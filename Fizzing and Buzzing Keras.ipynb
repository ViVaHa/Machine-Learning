{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic Based FizzBuzz Function [Software 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizzbuzz(n):\n",
    "    \n",
    "    # if n is divisible by 3 and 5 we return the string FizBuzz\n",
    "    if n % 3 == 0 and n % 5 == 0:\n",
    "        return 'FizzBuzz'\n",
    "    #if n is divisible by 3 only we return the string Fizz\n",
    "    elif n % 3 == 0:\n",
    "        return 'Fizz'\n",
    "    #if n is divisible by 5 only we return the string Buzz\n",
    "    elif n % 5 == 0:\n",
    "        return 'Buzz'\n",
    "    #if n is not divisible by 3 or 5 we return the string Other as default\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Testing Datasets in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInputCSV(start,end,filename):\n",
    "    \n",
    "    # Why list in Python?\n",
    "    '''\n",
    "    We use lists in Python to store the data. Data can be of any type and so lists are helpful\n",
    "    '''\n",
    "    inputData   = []\n",
    "    outputData  = []\n",
    "    \n",
    "    # Why do we need training Data?\n",
    "    '''\n",
    "    We need some reference that the machine can use to learn. Without training data it would be impossible\n",
    "    for a computer to predict something.\n",
    "    '''\n",
    "    \n",
    "    for i in range(start,end):\n",
    "        inputData.append(i)\n",
    "        outputData.append(fizzbuzz(i))\n",
    "    \n",
    "    # Why Dataframe?\n",
    "    '''\n",
    "    We use data frames because it aligns data neatly in the form of rows and columns. Making operations on an entire\n",
    "    row and column is easy with data frames\n",
    "    '''\n",
    "    dataset = {}\n",
    "    dataset[\"input\"]  = inputData\n",
    "    dataset[\"label\"] = outputData\n",
    "    \n",
    "    # Writing to csv\n",
    "    pd.DataFrame(dataset).to_csv(filename)\n",
    "    \n",
    "    print(filename, \"Created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Input and Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(dataset):\n",
    "    \n",
    "    # Why do we have to process?\n",
    "    '''\n",
    "    Here we preprocess the data to make sure it is in a language that the machine can understand. The data is converted\n",
    "    to its binary format and the labels are also encoded in a way that will be explained below.\n",
    "    '''\n",
    "    data   = dataset['input'].values\n",
    "    labels = dataset['label'].values\n",
    "    '''\n",
    "    Calls to methods to encode the data and labels\n",
    "    '''\n",
    "    processedData  = encodeData(data)\n",
    "    processedLabel = encodeLabel(labels)\n",
    "    \n",
    "    return processedData, processedLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def encodeData(data):\n",
    "    \n",
    "    processedData = []\n",
    "    '''\n",
    "    For each data point we perform a right shift and a logical AND to determine if a particular bit is set in its binary\n",
    "    format. We do this 10 times because we have a little below 1000 numbers in our data set and to represent the nearest\n",
    "    power of 2 with this number requires 10 bits(1024).\n",
    "    '''\n",
    "    for dataInstance in data:\n",
    "        \n",
    "        processedData.append([dataInstance >> d & 1 for d in range(10)])\n",
    "    \n",
    "    return np.array(processedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeLabel(labels):\n",
    "    \n",
    "    processedLabel = []\n",
    "    \n",
    "    '''\n",
    "    We also encode the labels so that the computer can understand it. We encode it in a similar way to how the\n",
    "    OneHotEncoder does (ie) each category will have a bit set on different columns.\n",
    "    \n",
    "    For example if there are three categories, the representation will look like:\n",
    "    \n",
    "    Category 1 - 0 0 1\n",
    "    \n",
    "    Category 2 - 0 1 0\n",
    "    \n",
    "    Category 3 - 1 0 0\n",
    "    \n",
    "    '''\n",
    "    for labelInstance in labels:\n",
    "        if(labelInstance == \"FizzBuzz\"):\n",
    "            # Fizzbuzz\n",
    "            processedLabel.append([3])\n",
    "        elif(labelInstance == \"Fizz\"):\n",
    "            # Fizz\n",
    "            processedLabel.append([1])\n",
    "        elif(labelInstance == \"Buzz\"):\n",
    "            # Buzz\n",
    "            processedLabel.append([2])\n",
    "        else:\n",
    "            # Other\n",
    "            processedLabel.append([0])\n",
    "\n",
    "    return np_utils.to_categorical(np.array(processedLabel),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "input_size = 10\n",
    "drop_out = 0.2\n",
    "first_dense_layer_nodes  = 1024\n",
    "second_dense_layer_nodes = 4\n",
    "\n",
    "def get_model():\n",
    "    \n",
    "    # Why do we need a model?\n",
    "    '''\n",
    "    We need a model to specify the configuration parameters such as the type of optimizer we require, the activation \n",
    "    function that we need etc.\n",
    "    '''\n",
    "    # Why use Dense layer and then activation?\n",
    "    '''\n",
    "    DENSE LAYER:\n",
    "    Dense layer is simply a layer where each unit or neuron is connected to each neuron in the next layer. This forms \n",
    "    the basis of a neural network. Each neuron recieves input from all the neurons in the previous layer, \n",
    "    thus densely connected. \n",
    "    \n",
    "    ACTIVATION:\n",
    "    Essentially each neuron will not know the bounds of the values coming in and the goal of a neuron is to get \n",
    "    activated or not.To make this into a Yes (or) No decision for a neuron to fire we make use of the activation \n",
    "    function which brings the values between 0 and 1 (or) between -1 and 1 depending on the type of function we choose.\n",
    "    Simply put, it converts the incoming values into a language that the neuron can understand.\n",
    "    '''\n",
    "    # Why use sequential model with layers?\n",
    "    '''\n",
    "    A model defines the relationship between features and labels.\n",
    "    Sequential model helps us to provide a layer of neurons one after the other in the form of a sequence. If we \n",
    "    require mutliple levels of interconnected neurons the sequential model can help us by allowing us to keep \n",
    "    stacking a bunch of neuron layers one after the other.\n",
    "    \n",
    "    We use a dense layer to do the Wi*Xi (ie) multiply the input matrix by the weight vector. From the official \n",
    "    documentation:\n",
    "    \n",
    "    Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise \n",
    "    activation function passed as the activation argument, kernel is a weights matrix created by the layer, \n",
    "    and bias is a bias vector created by the layer\n",
    "    \n",
    "    So, We can specify how we want to initialize the weights, it also allows us to specify the bias term\n",
    "    (similar to an intercept of a straight line).\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    model = Sequential()\n",
    "    '''\n",
    "    We add a level of neurons to do the processing for us. \n",
    "    '''\n",
    "    model.add(Dense(first_dense_layer_nodes, input_dim=input_size))\n",
    "    \n",
    "    '''\n",
    "    We use relu instead of functions like tanh and sigmoid because they have the vanishing gradient problem\n",
    "    (ie) after a certain point if there are changes in the input, the output remains the same which means \n",
    "    network refuses to learn for any change in X\n",
    "    '''\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Why dropout?\n",
    "    '''\n",
    "    Dropout is ignoring units (i.e. neurons) during the training phase of certain set of neurons which is chosen at \n",
    "    random. Dropout is done to prevent over fitting (ie) drawing a curve that describes only the training data and \n",
    "    is useless at generalizing data. So unseen data which may be the test data might be incorrectly predicted.\n",
    "    '''\n",
    "    model.add(Dropout(drop_out))\n",
    "    '''\n",
    "    This is the output layer of neurons which is equal to the number of classes we have.\n",
    "    '''\n",
    "    model.add(Dense(second_dense_layer_nodes))\n",
    "    # Why Softmax?\n",
    "    '''\n",
    "    The softmax function helps us to get a probability distribution where the class with a higher value will have high\n",
    "    probability. Softmax helps to approximate which doesnt not force the minimum value to 0 making it differentiable.\n",
    "    Softmax narrows down the range to bring it to between 0 and 1.\n",
    "    \n",
    "    '''\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    '''\n",
    "    We get the list of configs we set for the model.\n",
    "    '''\n",
    "    model.summary()\n",
    "    \n",
    "    # Why use categorical_crossentropy?\n",
    "    '''\n",
    "    The cross entropy function is more suited to classification problems which have non linear functions. This is \n",
    "    chosen over mean_squared_error function which is used for regression problems and functions which have linearity.\n",
    "    categorical_crossentropy is used to predict multiple mutually-exclusive classes.\n",
    "    \n",
    "    Cross entropy loss function is also called log loss which measures the performance of a classification model whose \n",
    "    output is a probability value between 0 and 1\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    We use an Adam optimizer with a learning rate of 0.001.\n",
    "    '''\n",
    "    opt=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Creating Training and Testing Datafiles</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training.csv Created!\n",
      "testing.csv Created!\n"
     ]
    }
   ],
   "source": [
    "# Create datafiles\n",
    "'''\n",
    "We create two seperate files one for training and one for testing\n",
    "'''\n",
    "createInputCSV(101,1001,'training.csv')\n",
    "createInputCSV(1,101,'testing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Creating Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_47 (Dense)             (None, 1024)              11264     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 4)                 4100      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 15,364\n",
      "Trainable params: 15,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue>Run Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 675 samples, validate on 225 samples\n",
      "Epoch 1/1000\n",
      "675/675 [==============================] - 0s 423us/step - loss: 1.3903 - acc: 0.2815 - val_loss: 1.2583 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 1.2393 - acc: 0.5363 - val_loss: 1.1672 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 1.1707 - acc: 0.5333 - val_loss: 1.1560 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 1.1526 - acc: 0.5333 - val_loss: 1.1750 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "675/675 [==============================] - 0s 35us/step - loss: 1.1603 - acc: 0.5333 - val_loss: 1.1826 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 1.1618 - acc: 0.5333 - val_loss: 1.1773 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 1.1542 - acc: 0.5333 - val_loss: 1.1679 - val_acc: 0.5333\n",
      "Epoch 8/1000\n",
      "675/675 [==============================] - 0s 32us/step - loss: 1.1484 - acc: 0.5333 - val_loss: 1.1590 - val_acc: 0.5333\n",
      "Epoch 9/1000\n",
      "675/675 [==============================] - 0s 32us/step - loss: 1.1454 - acc: 0.5333 - val_loss: 1.1521 - val_acc: 0.5333\n",
      "Epoch 10/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 1.1458 - acc: 0.5333 - val_loss: 1.1484 - val_acc: 0.5333\n",
      "Epoch 11/1000\n",
      "675/675 [==============================] - 0s 71us/step - loss: 1.1404 - acc: 0.5333 - val_loss: 1.1478 - val_acc: 0.5333\n",
      "Epoch 12/1000\n",
      "675/675 [==============================] - 0s 67us/step - loss: 1.1394 - acc: 0.5333 - val_loss: 1.1485 - val_acc: 0.5333\n",
      "Epoch 13/1000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 1.1348 - acc: 0.5333 - val_loss: 1.1475 - val_acc: 0.5333\n",
      "Epoch 14/1000\n",
      "675/675 [==============================] - 0s 46us/step - loss: 1.1379 - acc: 0.5333 - val_loss: 1.1468 - val_acc: 0.5333\n",
      "Epoch 15/1000\n",
      "675/675 [==============================] - 0s 34us/step - loss: 1.1347 - acc: 0.5333 - val_loss: 1.1459 - val_acc: 0.5333\n",
      "Epoch 16/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 1.1332 - acc: 0.5333 - val_loss: 1.1464 - val_acc: 0.5333\n",
      "Epoch 17/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 1.1311 - acc: 0.5333 - val_loss: 1.1468 - val_acc: 0.5333\n",
      "Epoch 18/1000\n",
      "675/675 [==============================] - 0s 39us/step - loss: 1.1295 - acc: 0.5333 - val_loss: 1.1475 - val_acc: 0.5333\n",
      "Epoch 19/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 1.1287 - acc: 0.5333 - val_loss: 1.1473 - val_acc: 0.5333\n",
      "Epoch 20/1000\n",
      "675/675 [==============================] - 0s 35us/step - loss: 1.1278 - acc: 0.5333 - val_loss: 1.1473 - val_acc: 0.5333\n",
      "Epoch 21/1000\n",
      "675/675 [==============================] - 0s 38us/step - loss: 1.1237 - acc: 0.5333 - val_loss: 1.1471 - val_acc: 0.5333\n",
      "Epoch 22/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 1.1224 - acc: 0.5333 - val_loss: 1.1469 - val_acc: 0.5333\n",
      "Epoch 23/1000\n",
      "675/675 [==============================] - 0s 36us/step - loss: 1.1227 - acc: 0.5333 - val_loss: 1.1480 - val_acc: 0.5333\n",
      "Epoch 24/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 1.1175 - acc: 0.5333 - val_loss: 1.1483 - val_acc: 0.5333\n",
      "Epoch 25/1000\n",
      "675/675 [==============================] - 0s 36us/step - loss: 1.1181 - acc: 0.5333 - val_loss: 1.1479 - val_acc: 0.5333\n",
      "Epoch 26/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 1.1172 - acc: 0.5333 - val_loss: 1.1471 - val_acc: 0.5333\n",
      "Epoch 27/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 1.1171 - acc: 0.5333 - val_loss: 1.1479 - val_acc: 0.5333\n",
      "Epoch 28/1000\n",
      "675/675 [==============================] - 0s 35us/step - loss: 1.1120 - acc: 0.5333 - val_loss: 1.1477 - val_acc: 0.5333\n",
      "Epoch 29/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 1.1115 - acc: 0.5333 - val_loss: 1.1475 - val_acc: 0.5333\n",
      "Epoch 30/1000\n",
      "675/675 [==============================] - 0s 32us/step - loss: 1.1127 - acc: 0.5333 - val_loss: 1.1462 - val_acc: 0.5333\n",
      "Epoch 31/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 1.1062 - acc: 0.5333 - val_loss: 1.1457 - val_acc: 0.5333\n",
      "Epoch 32/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 1.1062 - acc: 0.5333 - val_loss: 1.1454 - val_acc: 0.5333\n",
      "Epoch 33/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 1.1036 - acc: 0.5333 - val_loss: 1.1451 - val_acc: 0.5333\n",
      "Epoch 34/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 1.1030 - acc: 0.5333 - val_loss: 1.1452 - val_acc: 0.5333\n",
      "Epoch 35/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 1.1012 - acc: 0.5333 - val_loss: 1.1441 - val_acc: 0.5333\n",
      "Epoch 36/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 1.0945 - acc: 0.5333 - val_loss: 1.1437 - val_acc: 0.5333\n",
      "Epoch 37/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 1.0932 - acc: 0.5333 - val_loss: 1.1438 - val_acc: 0.5333\n",
      "Epoch 38/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 1.0923 - acc: 0.5333 - val_loss: 1.1439 - val_acc: 0.5333\n",
      "Epoch 39/1000\n",
      "675/675 [==============================] - 0s 34us/step - loss: 1.0903 - acc: 0.5333 - val_loss: 1.1436 - val_acc: 0.5333\n",
      "Epoch 40/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 1.0878 - acc: 0.5333 - val_loss: 1.1426 - val_acc: 0.5333\n",
      "Epoch 41/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 1.0860 - acc: 0.5333 - val_loss: 1.1422 - val_acc: 0.5333\n",
      "Epoch 42/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 1.0822 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 43/1000\n",
      "675/675 [==============================] - 0s 32us/step - loss: 1.0778 - acc: 0.5333 - val_loss: 1.1405 - val_acc: 0.5333\n",
      "Epoch 44/1000\n",
      "675/675 [==============================] - 0s 34us/step - loss: 1.0781 - acc: 0.5333 - val_loss: 1.1399 - val_acc: 0.5333\n",
      "Epoch 45/1000\n",
      "675/675 [==============================] - 0s 39us/step - loss: 1.0744 - acc: 0.5333 - val_loss: 1.1391 - val_acc: 0.5333\n",
      "Epoch 46/1000\n",
      "675/675 [==============================] - 0s 35us/step - loss: 1.0730 - acc: 0.5333 - val_loss: 1.1381 - val_acc: 0.5378\n",
      "Epoch 47/1000\n",
      "675/675 [==============================] - 0s 41us/step - loss: 1.0687 - acc: 0.5348 - val_loss: 1.1381 - val_acc: 0.5378\n",
      "Epoch 48/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 1.0658 - acc: 0.5348 - val_loss: 1.1382 - val_acc: 0.5378\n",
      "Epoch 49/1000\n",
      "675/675 [==============================] - 0s 34us/step - loss: 1.0607 - acc: 0.5378 - val_loss: 1.1373 - val_acc: 0.5378\n",
      "Epoch 50/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 1.0596 - acc: 0.5363 - val_loss: 1.1372 - val_acc: 0.5333\n",
      "Epoch 51/1000\n",
      "675/675 [==============================] - 0s 35us/step - loss: 1.0585 - acc: 0.5378 - val_loss: 1.1370 - val_acc: 0.5333\n",
      "Epoch 52/1000\n",
      "675/675 [==============================] - 0s 39us/step - loss: 1.0521 - acc: 0.5393 - val_loss: 1.1351 - val_acc: 0.5333\n",
      "Epoch 53/1000\n",
      "675/675 [==============================] - 0s 35us/step - loss: 1.0507 - acc: 0.5422 - val_loss: 1.1344 - val_acc: 0.5333\n",
      "Epoch 54/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 1.0448 - acc: 0.5378 - val_loss: 1.1338 - val_acc: 0.5333\n",
      "Epoch 55/1000\n",
      "675/675 [==============================] - 0s 36us/step - loss: 1.0412 - acc: 0.5467 - val_loss: 1.1331 - val_acc: 0.5378\n",
      "Epoch 56/1000\n",
      "675/675 [==============================] - 0s 37us/step - loss: 1.0365 - acc: 0.5407 - val_loss: 1.1317 - val_acc: 0.5422\n",
      "Epoch 57/1000\n",
      "675/675 [==============================] - 0s 35us/step - loss: 1.0395 - acc: 0.5526 - val_loss: 1.1309 - val_acc: 0.5378\n",
      "Epoch 58/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 1.0322 - acc: 0.5467 - val_loss: 1.1286 - val_acc: 0.5378\n",
      "Epoch 59/1000\n",
      "675/675 [==============================] - 0s 37us/step - loss: 1.0302 - acc: 0.5407 - val_loss: 1.1272 - val_acc: 0.5333\n",
      "Epoch 60/1000\n",
      "675/675 [==============================] - 0s 36us/step - loss: 1.0244 - acc: 0.5407 - val_loss: 1.1259 - val_acc: 0.5378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000\n",
      "675/675 [==============================] - 0s 37us/step - loss: 1.0184 - acc: 0.5481 - val_loss: 1.1249 - val_acc: 0.5422\n",
      "Epoch 62/1000\n",
      "675/675 [==============================] - 0s 37us/step - loss: 1.0173 - acc: 0.5511 - val_loss: 1.1236 - val_acc: 0.5422\n",
      "Epoch 63/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 1.0110 - acc: 0.5467 - val_loss: 1.1225 - val_acc: 0.5422\n",
      "Epoch 64/1000\n",
      "675/675 [==============================] - 0s 32us/step - loss: 1.0101 - acc: 0.5481 - val_loss: 1.1210 - val_acc: 0.5378\n",
      "Epoch 65/1000\n",
      "675/675 [==============================] - 0s 39us/step - loss: 1.0038 - acc: 0.5526 - val_loss: 1.1196 - val_acc: 0.5422\n",
      "Epoch 66/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.9997 - acc: 0.5541 - val_loss: 1.1175 - val_acc: 0.5378\n",
      "Epoch 67/1000\n",
      "675/675 [==============================] - 0s 35us/step - loss: 0.9968 - acc: 0.5481 - val_loss: 1.1158 - val_acc: 0.5378\n",
      "Epoch 68/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 0.9927 - acc: 0.5556 - val_loss: 1.1152 - val_acc: 0.5333\n",
      "Epoch 69/1000\n",
      "675/675 [==============================] - 0s 35us/step - loss: 0.9901 - acc: 0.5659 - val_loss: 1.1137 - val_acc: 0.5333\n",
      "Epoch 70/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.9841 - acc: 0.5585 - val_loss: 1.1112 - val_acc: 0.5378\n",
      "Epoch 71/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.9814 - acc: 0.5615 - val_loss: 1.1110 - val_acc: 0.5289\n",
      "Epoch 72/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.9760 - acc: 0.5630 - val_loss: 1.1092 - val_acc: 0.5333\n",
      "Epoch 73/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.9717 - acc: 0.5659 - val_loss: 1.1069 - val_acc: 0.5333\n",
      "Epoch 74/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.9671 - acc: 0.5585 - val_loss: 1.1064 - val_acc: 0.5333\n",
      "Epoch 75/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.9633 - acc: 0.5704 - val_loss: 1.1061 - val_acc: 0.5467\n",
      "Epoch 76/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.9582 - acc: 0.5733 - val_loss: 1.1031 - val_acc: 0.5467\n",
      "Epoch 77/1000\n",
      "675/675 [==============================] - 0s 35us/step - loss: 0.9564 - acc: 0.5733 - val_loss: 1.1016 - val_acc: 0.5467\n",
      "Epoch 78/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.9520 - acc: 0.5748 - val_loss: 1.1011 - val_acc: 0.5333\n",
      "Epoch 79/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.9498 - acc: 0.5778 - val_loss: 1.0986 - val_acc: 0.5422\n",
      "Epoch 80/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.9429 - acc: 0.5852 - val_loss: 1.0947 - val_acc: 0.5556\n",
      "Epoch 81/1000\n",
      "675/675 [==============================] - 0s 32us/step - loss: 0.9388 - acc: 0.5763 - val_loss: 1.0925 - val_acc: 0.5644\n",
      "Epoch 82/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.9335 - acc: 0.5704 - val_loss: 1.0920 - val_acc: 0.5600\n",
      "Epoch 83/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.9309 - acc: 0.5719 - val_loss: 1.0891 - val_acc: 0.5644\n",
      "Epoch 84/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.9232 - acc: 0.5778 - val_loss: 1.0888 - val_acc: 0.5422\n",
      "Epoch 85/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.9157 - acc: 0.5793 - val_loss: 1.0853 - val_acc: 0.5511\n",
      "Epoch 86/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.9152 - acc: 0.5867 - val_loss: 1.0848 - val_acc: 0.5333\n",
      "Epoch 87/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.9136 - acc: 0.5911 - val_loss: 1.0829 - val_acc: 0.5378\n",
      "Epoch 88/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.9086 - acc: 0.5793 - val_loss: 1.0817 - val_acc: 0.5289\n",
      "Epoch 89/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.8977 - acc: 0.6000 - val_loss: 1.0771 - val_acc: 0.5467\n",
      "Epoch 90/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.8920 - acc: 0.5896 - val_loss: 1.0750 - val_acc: 0.5467\n",
      "Epoch 91/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.8952 - acc: 0.5941 - val_loss: 1.0717 - val_acc: 0.5556\n",
      "Epoch 92/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.8868 - acc: 0.5867 - val_loss: 1.0682 - val_acc: 0.5556\n",
      "Epoch 93/1000\n",
      "675/675 [==============================] - 0s 32us/step - loss: 0.8823 - acc: 0.5896 - val_loss: 1.0663 - val_acc: 0.5556\n",
      "Epoch 94/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.8802 - acc: 0.5985 - val_loss: 1.0675 - val_acc: 0.5289\n",
      "Epoch 95/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.8711 - acc: 0.6133 - val_loss: 1.0642 - val_acc: 0.5378\n",
      "Epoch 96/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.8680 - acc: 0.6119 - val_loss: 1.0613 - val_acc: 0.5467\n",
      "Epoch 97/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 0.8691 - acc: 0.5881 - val_loss: 1.0593 - val_acc: 0.5511\n",
      "Epoch 98/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.8601 - acc: 0.6030 - val_loss: 1.0585 - val_acc: 0.5467\n",
      "Epoch 99/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.8611 - acc: 0.6044 - val_loss: 1.0547 - val_acc: 0.5600\n",
      "Epoch 100/1000\n",
      "675/675 [==============================] - 0s 32us/step - loss: 0.8508 - acc: 0.6104 - val_loss: 1.0518 - val_acc: 0.5600\n",
      "Epoch 101/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.8487 - acc: 0.6163 - val_loss: 1.0510 - val_acc: 0.5289\n",
      "Epoch 102/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.8325 - acc: 0.6370 - val_loss: 1.0482 - val_acc: 0.5467\n",
      "Epoch 103/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.8370 - acc: 0.6237 - val_loss: 1.0486 - val_acc: 0.5200\n",
      "Epoch 104/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.8332 - acc: 0.6370 - val_loss: 1.0440 - val_acc: 0.5378\n",
      "Epoch 105/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.8307 - acc: 0.6193 - val_loss: 1.0424 - val_acc: 0.5378\n",
      "Epoch 106/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.8232 - acc: 0.6148 - val_loss: 1.0407 - val_acc: 0.5333\n",
      "Epoch 107/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.8198 - acc: 0.6326 - val_loss: 1.0418 - val_acc: 0.5244\n",
      "Epoch 108/1000\n",
      "675/675 [==============================] - 0s 35us/step - loss: 0.8123 - acc: 0.6474 - val_loss: 1.0390 - val_acc: 0.5289\n",
      "Epoch 109/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.8027 - acc: 0.6459 - val_loss: 1.0344 - val_acc: 0.5422\n",
      "Epoch 110/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.7978 - acc: 0.6533 - val_loss: 1.0300 - val_acc: 0.5422\n",
      "Epoch 111/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.7979 - acc: 0.6474 - val_loss: 1.0268 - val_acc: 0.5422\n",
      "Epoch 112/1000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 0.7873 - acc: 0.6593 - val_loss: 1.0237 - val_acc: 0.5333\n",
      "Epoch 113/1000\n",
      "675/675 [==============================] - 0s 38us/step - loss: 0.7802 - acc: 0.6563 - val_loss: 1.0204 - val_acc: 0.5422\n",
      "Epoch 114/1000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 0.7763 - acc: 0.6741 - val_loss: 1.0191 - val_acc: 0.5467\n",
      "Epoch 115/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.7746 - acc: 0.6563 - val_loss: 1.0179 - val_acc: 0.5556\n",
      "Epoch 116/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.7810 - acc: 0.6578 - val_loss: 1.0150 - val_acc: 0.5556\n",
      "Epoch 117/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.7706 - acc: 0.6756 - val_loss: 1.0106 - val_acc: 0.5600\n",
      "Epoch 118/1000\n",
      "675/675 [==============================] - 0s 32us/step - loss: 0.7607 - acc: 0.6904 - val_loss: 1.0063 - val_acc: 0.5556\n",
      "Epoch 119/1000\n",
      "675/675 [==============================] - 0s 36us/step - loss: 0.7607 - acc: 0.6770 - val_loss: 1.0036 - val_acc: 0.5689\n",
      "Epoch 120/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.7523 - acc: 0.6859 - val_loss: 1.0004 - val_acc: 0.5644\n",
      "Epoch 121/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 21us/step - loss: 0.7495 - acc: 0.6696 - val_loss: 0.9971 - val_acc: 0.5689\n",
      "Epoch 122/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.7380 - acc: 0.6904 - val_loss: 0.9939 - val_acc: 0.5733\n",
      "Epoch 123/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.7401 - acc: 0.6933 - val_loss: 0.9915 - val_acc: 0.5644\n",
      "Epoch 124/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.7287 - acc: 0.7067 - val_loss: 0.9902 - val_acc: 0.5733\n",
      "Epoch 125/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.7263 - acc: 0.7007 - val_loss: 0.9888 - val_acc: 0.5733\n",
      "Epoch 126/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.7220 - acc: 0.7244 - val_loss: 0.9879 - val_acc: 0.5822\n",
      "Epoch 127/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.7168 - acc: 0.7215 - val_loss: 0.9836 - val_acc: 0.5733\n",
      "Epoch 128/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.7031 - acc: 0.7422 - val_loss: 0.9784 - val_acc: 0.5778\n",
      "Epoch 129/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.7093 - acc: 0.7200 - val_loss: 0.9739 - val_acc: 0.5733\n",
      "Epoch 130/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.6976 - acc: 0.7363 - val_loss: 0.9743 - val_acc: 0.5778\n",
      "Epoch 131/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.7002 - acc: 0.7348 - val_loss: 0.9716 - val_acc: 0.5778\n",
      "Epoch 132/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.6898 - acc: 0.7541 - val_loss: 0.9677 - val_acc: 0.5778\n",
      "Epoch 133/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.6871 - acc: 0.7407 - val_loss: 0.9651 - val_acc: 0.5778\n",
      "Epoch 134/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.6888 - acc: 0.7363 - val_loss: 0.9634 - val_acc: 0.5822\n",
      "Epoch 135/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.6868 - acc: 0.7719 - val_loss: 0.9608 - val_acc: 0.5822\n",
      "Epoch 136/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.6661 - acc: 0.7733 - val_loss: 0.9566 - val_acc: 0.5733\n",
      "Epoch 137/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.6742 - acc: 0.7496 - val_loss: 0.9527 - val_acc: 0.5956\n",
      "Epoch 138/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.6642 - acc: 0.7378 - val_loss: 0.9528 - val_acc: 0.5733\n",
      "Epoch 139/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 0.6697 - acc: 0.7570 - val_loss: 0.9509 - val_acc: 0.5778\n",
      "Epoch 140/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.6555 - acc: 0.7837 - val_loss: 0.9467 - val_acc: 0.5778\n",
      "Epoch 141/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.6446 - acc: 0.7748 - val_loss: 0.9436 - val_acc: 0.5822\n",
      "Epoch 142/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.6496 - acc: 0.7585 - val_loss: 0.9439 - val_acc: 0.5822\n",
      "Epoch 143/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.6512 - acc: 0.7763 - val_loss: 0.9421 - val_acc: 0.5822\n",
      "Epoch 144/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.6342 - acc: 0.8000 - val_loss: 0.9372 - val_acc: 0.5867\n",
      "Epoch 145/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.6407 - acc: 0.7763 - val_loss: 0.9308 - val_acc: 0.5867\n",
      "Epoch 146/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.6236 - acc: 0.7896 - val_loss: 0.9301 - val_acc: 0.5867\n",
      "Epoch 147/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.6239 - acc: 0.7970 - val_loss: 0.9254 - val_acc: 0.5956\n",
      "Epoch 148/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.6225 - acc: 0.8074 - val_loss: 0.9218 - val_acc: 0.5956\n",
      "Epoch 149/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.6084 - acc: 0.8163 - val_loss: 0.9180 - val_acc: 0.5911\n",
      "Epoch 150/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.6167 - acc: 0.7985 - val_loss: 0.9141 - val_acc: 0.5956\n",
      "Epoch 151/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.6091 - acc: 0.8119 - val_loss: 0.9106 - val_acc: 0.6000\n",
      "Epoch 152/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.6075 - acc: 0.8104 - val_loss: 0.9091 - val_acc: 0.6133\n",
      "Epoch 153/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.5994 - acc: 0.8326 - val_loss: 0.9067 - val_acc: 0.6044\n",
      "Epoch 154/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.5957 - acc: 0.8163 - val_loss: 0.9060 - val_acc: 0.5956\n",
      "Epoch 155/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.5939 - acc: 0.8207 - val_loss: 0.9045 - val_acc: 0.5956\n",
      "Epoch 156/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.5819 - acc: 0.8370 - val_loss: 0.9021 - val_acc: 0.5956\n",
      "Epoch 157/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.5852 - acc: 0.8207 - val_loss: 0.8996 - val_acc: 0.6133\n",
      "Epoch 158/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.5723 - acc: 0.8207 - val_loss: 0.8958 - val_acc: 0.5956\n",
      "Epoch 159/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.5709 - acc: 0.8400 - val_loss: 0.8905 - val_acc: 0.5956\n",
      "Epoch 160/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.5721 - acc: 0.8400 - val_loss: 0.8857 - val_acc: 0.6044\n",
      "Epoch 161/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.5608 - acc: 0.8415 - val_loss: 0.8820 - val_acc: 0.6044\n",
      "Epoch 162/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.5623 - acc: 0.8415 - val_loss: 0.8791 - val_acc: 0.6267\n",
      "Epoch 163/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.5524 - acc: 0.8548 - val_loss: 0.8765 - val_acc: 0.6311\n",
      "Epoch 164/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.5434 - acc: 0.8533 - val_loss: 0.8730 - val_acc: 0.6222\n",
      "Epoch 165/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.5483 - acc: 0.8474 - val_loss: 0.8718 - val_acc: 0.6222\n",
      "Epoch 166/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.5372 - acc: 0.8593 - val_loss: 0.8705 - val_acc: 0.6356\n",
      "Epoch 167/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.5459 - acc: 0.8444 - val_loss: 0.8660 - val_acc: 0.6356\n",
      "Epoch 168/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.5322 - acc: 0.8593 - val_loss: 0.8608 - val_acc: 0.6400\n",
      "Epoch 169/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.5339 - acc: 0.8770 - val_loss: 0.8553 - val_acc: 0.6311\n",
      "Epoch 170/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.5230 - acc: 0.8622 - val_loss: 0.8518 - val_acc: 0.6444\n",
      "Epoch 171/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.5251 - acc: 0.8726 - val_loss: 0.8489 - val_acc: 0.6444\n",
      "Epoch 172/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.5331 - acc: 0.8578 - val_loss: 0.8487 - val_acc: 0.6667\n",
      "Epoch 173/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.5047 - acc: 0.8993 - val_loss: 0.8462 - val_acc: 0.6667\n",
      "Epoch 174/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.5101 - acc: 0.8815 - val_loss: 0.8417 - val_acc: 0.6533\n",
      "Epoch 175/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.5025 - acc: 0.8919 - val_loss: 0.8369 - val_acc: 0.6578\n",
      "Epoch 176/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.5039 - acc: 0.8741 - val_loss: 0.8338 - val_acc: 0.6533\n",
      "Epoch 177/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.4997 - acc: 0.8652 - val_loss: 0.8320 - val_acc: 0.6533\n",
      "Epoch 178/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.5077 - acc: 0.8785 - val_loss: 0.8310 - val_acc: 0.6711\n",
      "Epoch 179/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.4885 - acc: 0.8830 - val_loss: 0.8278 - val_acc: 0.6711\n",
      "Epoch 180/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.4921 - acc: 0.8711 - val_loss: 0.8264 - val_acc: 0.6756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.4792 - acc: 0.9052 - val_loss: 0.8249 - val_acc: 0.6756\n",
      "Epoch 182/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 0.4860 - acc: 0.8948 - val_loss: 0.8223 - val_acc: 0.6667\n",
      "Epoch 183/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.4703 - acc: 0.8993 - val_loss: 0.8181 - val_acc: 0.6667\n",
      "Epoch 184/1000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 0.4683 - acc: 0.8993 - val_loss: 0.8148 - val_acc: 0.6578\n",
      "Epoch 185/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 0.4691 - acc: 0.9052 - val_loss: 0.8116 - val_acc: 0.6711\n",
      "Epoch 186/1000\n",
      "675/675 [==============================] - 0s 34us/step - loss: 0.4535 - acc: 0.9200 - val_loss: 0.8076 - val_acc: 0.6844\n",
      "Epoch 187/1000\n",
      "675/675 [==============================] - 0s 43us/step - loss: 0.4599 - acc: 0.9052 - val_loss: 0.8038 - val_acc: 0.7022\n",
      "Epoch 188/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.4577 - acc: 0.9081 - val_loss: 0.8030 - val_acc: 0.7156\n",
      "Epoch 189/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.4463 - acc: 0.9244 - val_loss: 0.7993 - val_acc: 0.7067\n",
      "Epoch 190/1000\n",
      "675/675 [==============================] - 0s 35us/step - loss: 0.4462 - acc: 0.9052 - val_loss: 0.7982 - val_acc: 0.6800\n",
      "Epoch 191/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.4494 - acc: 0.9037 - val_loss: 0.7959 - val_acc: 0.6978\n",
      "Epoch 192/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.4448 - acc: 0.9126 - val_loss: 0.7897 - val_acc: 0.7156\n",
      "Epoch 193/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.4432 - acc: 0.9156 - val_loss: 0.7868 - val_acc: 0.7244\n",
      "Epoch 194/1000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 0.4420 - acc: 0.9274 - val_loss: 0.7826 - val_acc: 0.7289\n",
      "Epoch 195/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.4282 - acc: 0.9170 - val_loss: 0.7807 - val_acc: 0.6756\n",
      "Epoch 196/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.4299 - acc: 0.9007 - val_loss: 0.7762 - val_acc: 0.7067\n",
      "Epoch 197/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.4291 - acc: 0.9200 - val_loss: 0.7752 - val_acc: 0.6933\n",
      "Epoch 198/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.4254 - acc: 0.9156 - val_loss: 0.7742 - val_acc: 0.6933\n",
      "Epoch 199/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.4216 - acc: 0.9230 - val_loss: 0.7727 - val_acc: 0.6933\n",
      "Epoch 200/1000\n",
      "675/675 [==============================] - 0s 44us/step - loss: 0.4242 - acc: 0.9200 - val_loss: 0.7706 - val_acc: 0.7200\n",
      "Epoch 201/1000\n",
      "675/675 [==============================] - 0s 38us/step - loss: 0.4241 - acc: 0.9230 - val_loss: 0.7691 - val_acc: 0.7156\n",
      "Epoch 202/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.4142 - acc: 0.9081 - val_loss: 0.7684 - val_acc: 0.7022\n",
      "Epoch 203/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.4117 - acc: 0.9274 - val_loss: 0.7664 - val_acc: 0.7511\n",
      "Epoch 204/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.4103 - acc: 0.9363 - val_loss: 0.7628 - val_acc: 0.7067\n",
      "Epoch 205/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.4000 - acc: 0.9259 - val_loss: 0.7612 - val_acc: 0.7244\n",
      "Epoch 206/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.3975 - acc: 0.9244 - val_loss: 0.7575 - val_acc: 0.7378\n",
      "Epoch 207/1000\n",
      "675/675 [==============================] - 0s 66us/step - loss: 0.3949 - acc: 0.9348 - val_loss: 0.7551 - val_acc: 0.7422\n",
      "Epoch 208/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.4125 - acc: 0.9244 - val_loss: 0.7534 - val_acc: 0.7289\n",
      "Epoch 209/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.3965 - acc: 0.9259 - val_loss: 0.7495 - val_acc: 0.7467\n",
      "Epoch 210/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.3898 - acc: 0.9319 - val_loss: 0.7471 - val_acc: 0.7422\n",
      "Epoch 211/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.3916 - acc: 0.9274 - val_loss: 0.7439 - val_acc: 0.7422\n",
      "Epoch 212/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.3735 - acc: 0.9467 - val_loss: 0.7414 - val_acc: 0.7378\n",
      "Epoch 213/1000\n",
      "675/675 [==============================] - 0s 40us/step - loss: 0.3832 - acc: 0.9348 - val_loss: 0.7380 - val_acc: 0.7467\n",
      "Epoch 214/1000\n",
      "675/675 [==============================] - 0s 40us/step - loss: 0.3800 - acc: 0.9481 - val_loss: 0.7358 - val_acc: 0.7467\n",
      "Epoch 215/1000\n",
      "675/675 [==============================] - 0s 72us/step - loss: 0.3717 - acc: 0.9481 - val_loss: 0.7313 - val_acc: 0.7422\n",
      "Epoch 216/1000\n",
      "675/675 [==============================] - 0s 68us/step - loss: 0.3673 - acc: 0.9363 - val_loss: 0.7283 - val_acc: 0.7556\n",
      "Epoch 217/1000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 0.3790 - acc: 0.9348 - val_loss: 0.7272 - val_acc: 0.7600\n",
      "Epoch 218/1000\n",
      "675/675 [==============================] - 0s 41us/step - loss: 0.3704 - acc: 0.9481 - val_loss: 0.7259 - val_acc: 0.7511\n",
      "Epoch 219/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.3569 - acc: 0.9407 - val_loss: 0.7258 - val_acc: 0.7600\n",
      "Epoch 220/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.3675 - acc: 0.9452 - val_loss: 0.7264 - val_acc: 0.7689\n",
      "Epoch 221/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.3623 - acc: 0.9393 - val_loss: 0.7229 - val_acc: 0.7556\n",
      "Epoch 222/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.3680 - acc: 0.9422 - val_loss: 0.7193 - val_acc: 0.7556\n",
      "Epoch 223/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.3602 - acc: 0.9467 - val_loss: 0.7151 - val_acc: 0.7511\n",
      "Epoch 224/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.3561 - acc: 0.9393 - val_loss: 0.7120 - val_acc: 0.7689\n",
      "Epoch 225/1000\n",
      "675/675 [==============================] - 0s 66us/step - loss: 0.3437 - acc: 0.9541 - val_loss: 0.7091 - val_acc: 0.7600\n",
      "Epoch 226/1000\n",
      "675/675 [==============================] - 0s 36us/step - loss: 0.3339 - acc: 0.9467 - val_loss: 0.7077 - val_acc: 0.7644\n",
      "Epoch 227/1000\n",
      "675/675 [==============================] - 0s 39us/step - loss: 0.3555 - acc: 0.9481 - val_loss: 0.7052 - val_acc: 0.7689\n",
      "Epoch 228/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.3361 - acc: 0.9556 - val_loss: 0.7035 - val_acc: 0.7644\n",
      "Epoch 229/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.3446 - acc: 0.9348 - val_loss: 0.6982 - val_acc: 0.7778\n",
      "Epoch 230/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.3491 - acc: 0.9304 - val_loss: 0.6951 - val_acc: 0.7867\n",
      "Epoch 231/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.3424 - acc: 0.9526 - val_loss: 0.6935 - val_acc: 0.7911\n",
      "Epoch 232/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.3182 - acc: 0.9674 - val_loss: 0.6942 - val_acc: 0.7822\n",
      "Epoch 233/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.3285 - acc: 0.9437 - val_loss: 0.6914 - val_acc: 0.7822\n",
      "Epoch 234/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.3314 - acc: 0.9422 - val_loss: 0.6895 - val_acc: 0.7822\n",
      "Epoch 235/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.3197 - acc: 0.9422 - val_loss: 0.6887 - val_acc: 0.8000\n",
      "Epoch 236/1000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 0.3268 - acc: 0.9615 - val_loss: 0.6866 - val_acc: 0.7867\n",
      "Epoch 237/1000\n",
      "675/675 [==============================] - 0s 32us/step - loss: 0.3131 - acc: 0.9481 - val_loss: 0.6836 - val_acc: 0.7822\n",
      "Epoch 238/1000\n",
      "675/675 [==============================] - 0s 34us/step - loss: 0.3047 - acc: 0.9600 - val_loss: 0.6796 - val_acc: 0.7911\n",
      "Epoch 239/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.3197 - acc: 0.9570 - val_loss: 0.6751 - val_acc: 0.7956\n",
      "Epoch 240/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.3233 - acc: 0.9570 - val_loss: 0.6735 - val_acc: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.3121 - acc: 0.9615 - val_loss: 0.6740 - val_acc: 0.7778\n",
      "Epoch 242/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.3098 - acc: 0.9585 - val_loss: 0.6712 - val_acc: 0.7956\n",
      "Epoch 243/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.3095 - acc: 0.9570 - val_loss: 0.6694 - val_acc: 0.8044\n",
      "Epoch 244/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.3002 - acc: 0.9556 - val_loss: 0.6669 - val_acc: 0.8000\n",
      "Epoch 245/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.2971 - acc: 0.9570 - val_loss: 0.6664 - val_acc: 0.7867\n",
      "Epoch 246/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.3124 - acc: 0.9467 - val_loss: 0.6621 - val_acc: 0.7867\n",
      "Epoch 247/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.3005 - acc: 0.9496 - val_loss: 0.6599 - val_acc: 0.7911\n",
      "Epoch 248/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.2953 - acc: 0.9600 - val_loss: 0.6560 - val_acc: 0.7867\n",
      "Epoch 249/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.3042 - acc: 0.9585 - val_loss: 0.6539 - val_acc: 0.8044\n",
      "Epoch 250/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.2894 - acc: 0.9615 - val_loss: 0.6517 - val_acc: 0.8000\n",
      "Epoch 251/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.3024 - acc: 0.9511 - val_loss: 0.6495 - val_acc: 0.8133\n",
      "Epoch 252/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.2978 - acc: 0.9556 - val_loss: 0.6499 - val_acc: 0.8133\n",
      "Epoch 253/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.2840 - acc: 0.9644 - val_loss: 0.6498 - val_acc: 0.8044\n",
      "Epoch 254/1000\n",
      "675/675 [==============================] - 0s 43us/step - loss: 0.2884 - acc: 0.9674 - val_loss: 0.6510 - val_acc: 0.8000\n",
      "Epoch 255/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.2869 - acc: 0.9630 - val_loss: 0.6510 - val_acc: 0.7911\n",
      "Epoch 256/1000\n",
      "675/675 [==============================] - 0s 41us/step - loss: 0.2937 - acc: 0.9615 - val_loss: 0.6525 - val_acc: 0.7956\n",
      "Epoch 257/1000\n",
      "675/675 [==============================] - 0s 35us/step - loss: 0.2885 - acc: 0.9600 - val_loss: 0.6538 - val_acc: 0.7956\n",
      "Epoch 258/1000\n",
      "675/675 [==============================] - 0s 36us/step - loss: 0.2763 - acc: 0.9748 - val_loss: 0.6519 - val_acc: 0.8089\n",
      "Epoch 259/1000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 0.2817 - acc: 0.9689 - val_loss: 0.6513 - val_acc: 0.7867\n",
      "Epoch 260/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.2836 - acc: 0.9644 - val_loss: 0.6506 - val_acc: 0.7867\n",
      "Epoch 261/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.2709 - acc: 0.9630 - val_loss: 0.6414 - val_acc: 0.8089\n",
      "Epoch 262/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.2768 - acc: 0.9689 - val_loss: 0.6370 - val_acc: 0.8133\n",
      "Epoch 263/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.2755 - acc: 0.9644 - val_loss: 0.6362 - val_acc: 0.8133\n",
      "Epoch 264/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.2672 - acc: 0.9763 - val_loss: 0.6375 - val_acc: 0.7956\n",
      "Epoch 265/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.2772 - acc: 0.9526 - val_loss: 0.6362 - val_acc: 0.7956\n",
      "Epoch 266/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.2790 - acc: 0.9526 - val_loss: 0.6306 - val_acc: 0.8044\n",
      "Epoch 267/1000\n",
      "675/675 [==============================] - 0s 36us/step - loss: 0.2657 - acc: 0.9644 - val_loss: 0.6293 - val_acc: 0.8133\n",
      "Epoch 268/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.2771 - acc: 0.9422 - val_loss: 0.6293 - val_acc: 0.8133\n",
      "Epoch 269/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.2539 - acc: 0.9704 - val_loss: 0.6312 - val_acc: 0.8044\n",
      "Epoch 270/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.2531 - acc: 0.9644 - val_loss: 0.6247 - val_acc: 0.8222\n",
      "Epoch 271/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.2725 - acc: 0.9556 - val_loss: 0.6223 - val_acc: 0.8222\n",
      "Epoch 272/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.2533 - acc: 0.9748 - val_loss: 0.6272 - val_acc: 0.8133\n",
      "Epoch 273/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.2451 - acc: 0.9763 - val_loss: 0.6195 - val_acc: 0.8267\n",
      "Epoch 274/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.2569 - acc: 0.9600 - val_loss: 0.6161 - val_acc: 0.8267\n",
      "Epoch 275/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.2491 - acc: 0.9644 - val_loss: 0.6199 - val_acc: 0.8133\n",
      "Epoch 276/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.2565 - acc: 0.9600 - val_loss: 0.6183 - val_acc: 0.8222\n",
      "Epoch 277/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.2609 - acc: 0.9644 - val_loss: 0.6102 - val_acc: 0.8400\n",
      "Epoch 278/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.2600 - acc: 0.9630 - val_loss: 0.6077 - val_acc: 0.8356\n",
      "Epoch 279/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.2430 - acc: 0.9748 - val_loss: 0.6062 - val_acc: 0.8267\n",
      "Epoch 280/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.2386 - acc: 0.9689 - val_loss: 0.6073 - val_acc: 0.8267\n",
      "Epoch 281/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.2469 - acc: 0.9719 - val_loss: 0.6085 - val_acc: 0.8222\n",
      "Epoch 282/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.2402 - acc: 0.9644 - val_loss: 0.6086 - val_acc: 0.8178\n",
      "Epoch 283/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.2276 - acc: 0.9807 - val_loss: 0.6037 - val_acc: 0.8267\n",
      "Epoch 284/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.2398 - acc: 0.9600 - val_loss: 0.5990 - val_acc: 0.8267\n",
      "Epoch 285/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.2369 - acc: 0.9644 - val_loss: 0.5960 - val_acc: 0.8400\n",
      "Epoch 286/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.2436 - acc: 0.9763 - val_loss: 0.5945 - val_acc: 0.8356\n",
      "Epoch 287/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.2376 - acc: 0.9674 - val_loss: 0.5947 - val_acc: 0.8356\n",
      "Epoch 288/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.2190 - acc: 0.9763 - val_loss: 0.5931 - val_acc: 0.8400\n",
      "Epoch 289/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.2281 - acc: 0.9733 - val_loss: 0.5974 - val_acc: 0.8356\n",
      "Epoch 290/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.2270 - acc: 0.9719 - val_loss: 0.5984 - val_acc: 0.8222\n",
      "Epoch 291/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.2231 - acc: 0.9719 - val_loss: 0.5966 - val_acc: 0.8356\n",
      "Epoch 292/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.2249 - acc: 0.9852 - val_loss: 0.5967 - val_acc: 0.8267\n",
      "Epoch 293/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.2210 - acc: 0.9748 - val_loss: 0.6005 - val_acc: 0.8222\n",
      "Epoch 294/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.2196 - acc: 0.9704 - val_loss: 0.5947 - val_acc: 0.8267\n",
      "Epoch 295/1000\n",
      "675/675 [==============================] - 0s 34us/step - loss: 0.2231 - acc: 0.9704 - val_loss: 0.5909 - val_acc: 0.8356\n",
      "Epoch 296/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.2170 - acc: 0.9748 - val_loss: 0.5947 - val_acc: 0.8311\n",
      "Epoch 297/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.2335 - acc: 0.9644 - val_loss: 0.5932 - val_acc: 0.8311\n",
      "Epoch 298/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.2099 - acc: 0.9748 - val_loss: 0.5858 - val_acc: 0.8400\n",
      "Epoch 299/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.2209 - acc: 0.9793 - val_loss: 0.5866 - val_acc: 0.8311\n",
      "Epoch 300/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.2214 - acc: 0.9659 - val_loss: 0.5830 - val_acc: 0.8356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.2054 - acc: 0.9793 - val_loss: 0.5807 - val_acc: 0.8400\n",
      "Epoch 302/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.2161 - acc: 0.9719 - val_loss: 0.5792 - val_acc: 0.8489\n",
      "Epoch 303/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.2061 - acc: 0.9807 - val_loss: 0.5768 - val_acc: 0.8489\n",
      "Epoch 304/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.2104 - acc: 0.9807 - val_loss: 0.5747 - val_acc: 0.8533\n",
      "Epoch 305/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.2193 - acc: 0.9689 - val_loss: 0.5797 - val_acc: 0.8444\n",
      "Epoch 306/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.2078 - acc: 0.9793 - val_loss: 0.5764 - val_acc: 0.8400\n",
      "Epoch 307/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.2115 - acc: 0.9763 - val_loss: 0.5751 - val_acc: 0.8356\n",
      "Epoch 308/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.2232 - acc: 0.9600 - val_loss: 0.5723 - val_acc: 0.8400\n",
      "Epoch 309/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1982 - acc: 0.9763 - val_loss: 0.5670 - val_acc: 0.8578\n",
      "Epoch 310/1000\n",
      "675/675 [==============================] - 0s 121us/step - loss: 0.2086 - acc: 0.9793 - val_loss: 0.5665 - val_acc: 0.8489\n",
      "Epoch 311/1000\n",
      "675/675 [==============================] - 0s 37us/step - loss: 0.1949 - acc: 0.9807 - val_loss: 0.5631 - val_acc: 0.8444\n",
      "Epoch 312/1000\n",
      "675/675 [==============================] - 0s 40us/step - loss: 0.2095 - acc: 0.9659 - val_loss: 0.5608 - val_acc: 0.8533\n",
      "Epoch 313/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.2007 - acc: 0.9822 - val_loss: 0.5614 - val_acc: 0.8489\n",
      "Epoch 314/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.1905 - acc: 0.9763 - val_loss: 0.5614 - val_acc: 0.8489\n",
      "Epoch 315/1000\n",
      "675/675 [==============================] - 0s 39us/step - loss: 0.1975 - acc: 0.9778 - val_loss: 0.5581 - val_acc: 0.8444\n",
      "Epoch 316/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.1922 - acc: 0.9748 - val_loss: 0.5634 - val_acc: 0.8400\n",
      "Epoch 317/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.2001 - acc: 0.9644 - val_loss: 0.5615 - val_acc: 0.8356\n",
      "Epoch 318/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.1974 - acc: 0.9733 - val_loss: 0.5590 - val_acc: 0.8533\n",
      "Epoch 319/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.1923 - acc: 0.9807 - val_loss: 0.5588 - val_acc: 0.8400\n",
      "Epoch 320/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.1902 - acc: 0.9748 - val_loss: 0.5587 - val_acc: 0.8444\n",
      "Epoch 321/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.1900 - acc: 0.9763 - val_loss: 0.5607 - val_acc: 0.8400\n",
      "Epoch 322/1000\n",
      "675/675 [==============================] - 0s 39us/step - loss: 0.1984 - acc: 0.9778 - val_loss: 0.5555 - val_acc: 0.8533\n",
      "Epoch 323/1000\n",
      "675/675 [==============================] - 0s 37us/step - loss: 0.1905 - acc: 0.9822 - val_loss: 0.5531 - val_acc: 0.8622\n",
      "Epoch 324/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.1969 - acc: 0.9719 - val_loss: 0.5565 - val_acc: 0.8489\n",
      "Epoch 325/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.1933 - acc: 0.9822 - val_loss: 0.5577 - val_acc: 0.8489\n",
      "Epoch 326/1000\n",
      "675/675 [==============================] - 0s 40us/step - loss: 0.1812 - acc: 0.9748 - val_loss: 0.5539 - val_acc: 0.8533\n",
      "Epoch 327/1000\n",
      "675/675 [==============================] - 0s 35us/step - loss: 0.1799 - acc: 0.9867 - val_loss: 0.5550 - val_acc: 0.8533\n",
      "Epoch 328/1000\n",
      "675/675 [==============================] - 0s 36us/step - loss: 0.1832 - acc: 0.9807 - val_loss: 0.5618 - val_acc: 0.8400\n",
      "Epoch 329/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 0.1853 - acc: 0.9793 - val_loss: 0.5630 - val_acc: 0.8356\n",
      "Epoch 330/1000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 0.1803 - acc: 0.9733 - val_loss: 0.5558 - val_acc: 0.8400\n",
      "Epoch 331/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.1926 - acc: 0.9704 - val_loss: 0.5480 - val_acc: 0.8489\n",
      "Epoch 332/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 0.1757 - acc: 0.9837 - val_loss: 0.5401 - val_acc: 0.8533\n",
      "Epoch 333/1000\n",
      "675/675 [==============================] - 0s 32us/step - loss: 0.1863 - acc: 0.9793 - val_loss: 0.5397 - val_acc: 0.8489\n",
      "Epoch 334/1000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 0.1828 - acc: 0.9807 - val_loss: 0.5424 - val_acc: 0.8533\n",
      "Epoch 335/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 0.1769 - acc: 0.9822 - val_loss: 0.5416 - val_acc: 0.8533\n",
      "Epoch 336/1000\n",
      "675/675 [==============================] - 0s 35us/step - loss: 0.1813 - acc: 0.9822 - val_loss: 0.5430 - val_acc: 0.8533\n",
      "Epoch 337/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.1684 - acc: 0.9837 - val_loss: 0.5425 - val_acc: 0.8578\n",
      "Epoch 338/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.1724 - acc: 0.9867 - val_loss: 0.5415 - val_acc: 0.8667\n",
      "Epoch 339/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.1684 - acc: 0.9837 - val_loss: 0.5465 - val_acc: 0.8444\n",
      "Epoch 340/1000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 0.1801 - acc: 0.9748 - val_loss: 0.5392 - val_acc: 0.8578\n",
      "Epoch 341/1000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 0.1866 - acc: 0.9659 - val_loss: 0.5383 - val_acc: 0.8578\n",
      "Epoch 342/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.1779 - acc: 0.9778 - val_loss: 0.5433 - val_acc: 0.8578\n",
      "Epoch 343/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.1717 - acc: 0.9763 - val_loss: 0.5425 - val_acc: 0.8578\n",
      "Epoch 344/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.1779 - acc: 0.9793 - val_loss: 0.5353 - val_acc: 0.8578\n",
      "Epoch 345/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.1684 - acc: 0.9793 - val_loss: 0.5335 - val_acc: 0.8578\n",
      "Epoch 346/1000\n",
      "675/675 [==============================] - 0s 68us/step - loss: 0.1644 - acc: 0.9867 - val_loss: 0.5500 - val_acc: 0.8222\n",
      "Epoch 347/1000\n",
      "675/675 [==============================] - 0s 32us/step - loss: 0.1713 - acc: 0.9763 - val_loss: 0.5424 - val_acc: 0.8356\n",
      "Epoch 348/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.1644 - acc: 0.9852 - val_loss: 0.5355 - val_acc: 0.8489\n",
      "Epoch 349/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.1730 - acc: 0.9763 - val_loss: 0.5369 - val_acc: 0.8533\n",
      "Epoch 350/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.1641 - acc: 0.9837 - val_loss: 0.5422 - val_acc: 0.8400\n",
      "Epoch 351/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.1563 - acc: 0.9911 - val_loss: 0.5327 - val_acc: 0.8622\n",
      "Epoch 352/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.1595 - acc: 0.9867 - val_loss: 0.5293 - val_acc: 0.8711\n",
      "Epoch 353/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.1705 - acc: 0.9822 - val_loss: 0.5322 - val_acc: 0.8578\n",
      "Epoch 354/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.1625 - acc: 0.9881 - val_loss: 0.5356 - val_acc: 0.8444\n",
      "Epoch 355/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1760 - acc: 0.9719 - val_loss: 0.5231 - val_acc: 0.8756\n",
      "Epoch 356/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1680 - acc: 0.9733 - val_loss: 0.5238 - val_acc: 0.8667\n",
      "Epoch 357/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.1796 - acc: 0.9704 - val_loss: 0.5281 - val_acc: 0.8489\n",
      "Epoch 358/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.1720 - acc: 0.9778 - val_loss: 0.5233 - val_acc: 0.8533\n",
      "Epoch 359/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.1623 - acc: 0.9807 - val_loss: 0.5207 - val_acc: 0.8667\n",
      "Epoch 360/1000\n",
      "675/675 [==============================] - 0s 37us/step - loss: 0.1526 - acc: 0.9881 - val_loss: 0.5226 - val_acc: 0.8622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/1000\n",
      "675/675 [==============================] - 0s 32us/step - loss: 0.1630 - acc: 0.9867 - val_loss: 0.5291 - val_acc: 0.8400\n",
      "Epoch 362/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.1569 - acc: 0.9822 - val_loss: 0.5228 - val_acc: 0.8622\n",
      "Epoch 363/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.1571 - acc: 0.9837 - val_loss: 0.5180 - val_acc: 0.8756\n",
      "Epoch 364/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.1513 - acc: 0.9852 - val_loss: 0.5313 - val_acc: 0.8533\n",
      "Epoch 365/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1649 - acc: 0.9778 - val_loss: 0.5332 - val_acc: 0.8400\n",
      "Epoch 366/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.1625 - acc: 0.9837 - val_loss: 0.5182 - val_acc: 0.8622\n",
      "Epoch 367/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.1544 - acc: 0.9852 - val_loss: 0.5194 - val_acc: 0.8622\n",
      "Epoch 368/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1498 - acc: 0.9807 - val_loss: 0.5302 - val_acc: 0.8356\n",
      "Epoch 369/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.1592 - acc: 0.9822 - val_loss: 0.5202 - val_acc: 0.8622\n",
      "Epoch 370/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1463 - acc: 0.9867 - val_loss: 0.5188 - val_acc: 0.8667\n",
      "Epoch 371/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1570 - acc: 0.9837 - val_loss: 0.5297 - val_acc: 0.8533\n",
      "Epoch 372/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 0.1508 - acc: 0.9867 - val_loss: 0.5217 - val_acc: 0.8578\n",
      "Epoch 373/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.1502 - acc: 0.9852 - val_loss: 0.5110 - val_acc: 0.8667\n",
      "Epoch 374/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1464 - acc: 0.9837 - val_loss: 0.5173 - val_acc: 0.8622\n",
      "Epoch 375/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.1451 - acc: 0.9881 - val_loss: 0.5249 - val_acc: 0.8667\n",
      "Epoch 376/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1467 - acc: 0.9793 - val_loss: 0.5151 - val_acc: 0.8667\n",
      "Epoch 377/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1498 - acc: 0.9852 - val_loss: 0.5101 - val_acc: 0.8711\n",
      "Epoch 378/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1522 - acc: 0.9807 - val_loss: 0.5061 - val_acc: 0.8667\n",
      "Epoch 379/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1391 - acc: 0.9911 - val_loss: 0.5138 - val_acc: 0.8711\n",
      "Epoch 380/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1446 - acc: 0.9778 - val_loss: 0.5053 - val_acc: 0.8711\n",
      "Epoch 381/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1307 - acc: 0.9911 - val_loss: 0.5031 - val_acc: 0.8667\n",
      "Epoch 382/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1350 - acc: 0.9881 - val_loss: 0.5045 - val_acc: 0.8667\n",
      "Epoch 383/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1349 - acc: 0.9881 - val_loss: 0.5023 - val_acc: 0.8711\n",
      "Epoch 384/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1411 - acc: 0.9778 - val_loss: 0.5050 - val_acc: 0.8667\n",
      "Epoch 385/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1422 - acc: 0.9793 - val_loss: 0.5020 - val_acc: 0.8667\n",
      "Epoch 386/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.1325 - acc: 0.9896 - val_loss: 0.4938 - val_acc: 0.8711\n",
      "Epoch 387/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.1474 - acc: 0.9793 - val_loss: 0.4940 - val_acc: 0.8756\n",
      "Epoch 388/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1399 - acc: 0.9793 - val_loss: 0.5074 - val_acc: 0.8667\n",
      "Epoch 389/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.1434 - acc: 0.9778 - val_loss: 0.5098 - val_acc: 0.8667\n",
      "Epoch 390/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1286 - acc: 0.9926 - val_loss: 0.5041 - val_acc: 0.8622\n",
      "Epoch 391/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1374 - acc: 0.9852 - val_loss: 0.5035 - val_acc: 0.8667\n",
      "Epoch 392/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1387 - acc: 0.9822 - val_loss: 0.5107 - val_acc: 0.8578\n",
      "Epoch 393/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1453 - acc: 0.9748 - val_loss: 0.5031 - val_acc: 0.8578\n",
      "Epoch 394/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1440 - acc: 0.9793 - val_loss: 0.4916 - val_acc: 0.8844\n",
      "Epoch 395/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1403 - acc: 0.9807 - val_loss: 0.4940 - val_acc: 0.8711\n",
      "Epoch 396/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1364 - acc: 0.9822 - val_loss: 0.4941 - val_acc: 0.8667\n",
      "Epoch 397/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1401 - acc: 0.9837 - val_loss: 0.4968 - val_acc: 0.8622\n",
      "Epoch 398/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1340 - acc: 0.9896 - val_loss: 0.4951 - val_acc: 0.8667\n",
      "Epoch 399/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1447 - acc: 0.9778 - val_loss: 0.4956 - val_acc: 0.8667\n",
      "Epoch 400/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1237 - acc: 0.9896 - val_loss: 0.4975 - val_acc: 0.8533\n",
      "Epoch 401/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1344 - acc: 0.9867 - val_loss: 0.4959 - val_acc: 0.8667\n",
      "Epoch 402/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1318 - acc: 0.9807 - val_loss: 0.5039 - val_acc: 0.8622\n",
      "Epoch 403/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.1349 - acc: 0.9852 - val_loss: 0.5066 - val_acc: 0.8622\n",
      "Epoch 404/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1290 - acc: 0.9867 - val_loss: 0.4955 - val_acc: 0.8667\n",
      "Epoch 405/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1236 - acc: 0.9911 - val_loss: 0.4963 - val_acc: 0.8667\n",
      "Epoch 406/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.1362 - acc: 0.9807 - val_loss: 0.4985 - val_acc: 0.8622\n",
      "Epoch 407/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1288 - acc: 0.9926 - val_loss: 0.4926 - val_acc: 0.8711\n",
      "Epoch 408/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.1279 - acc: 0.9881 - val_loss: 0.4987 - val_acc: 0.8533\n",
      "Epoch 409/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1167 - acc: 0.9852 - val_loss: 0.5041 - val_acc: 0.8533\n",
      "Epoch 410/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.1256 - acc: 0.9852 - val_loss: 0.4986 - val_acc: 0.8578\n",
      "Epoch 411/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1207 - acc: 0.9867 - val_loss: 0.4922 - val_acc: 0.8711\n",
      "Epoch 412/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1269 - acc: 0.9852 - val_loss: 0.4928 - val_acc: 0.8711\n",
      "Epoch 413/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1268 - acc: 0.9822 - val_loss: 0.4964 - val_acc: 0.8622\n",
      "Epoch 414/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1304 - acc: 0.9837 - val_loss: 0.4875 - val_acc: 0.8667\n",
      "Epoch 415/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1345 - acc: 0.9793 - val_loss: 0.4838 - val_acc: 0.8844\n",
      "Epoch 416/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1293 - acc: 0.9807 - val_loss: 0.4891 - val_acc: 0.8756\n",
      "Epoch 417/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.1324 - acc: 0.9793 - val_loss: 0.4848 - val_acc: 0.8800\n",
      "Epoch 418/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.1342 - acc: 0.9778 - val_loss: 0.4778 - val_acc: 0.8889\n",
      "Epoch 419/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1184 - acc: 0.9852 - val_loss: 0.4890 - val_acc: 0.8667\n",
      "Epoch 420/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1287 - acc: 0.9807 - val_loss: 0.4901 - val_acc: 0.8622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1421 - acc: 0.9689 - val_loss: 0.4921 - val_acc: 0.8622\n",
      "Epoch 422/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1164 - acc: 0.9867 - val_loss: 0.4920 - val_acc: 0.8711\n",
      "Epoch 423/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1208 - acc: 0.9822 - val_loss: 0.4855 - val_acc: 0.8756\n",
      "Epoch 424/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1247 - acc: 0.9881 - val_loss: 0.4801 - val_acc: 0.8711\n",
      "Epoch 425/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.1172 - acc: 0.9881 - val_loss: 0.4798 - val_acc: 0.8711\n",
      "Epoch 426/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1186 - acc: 0.9896 - val_loss: 0.4758 - val_acc: 0.8756\n",
      "Epoch 427/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1175 - acc: 0.9852 - val_loss: 0.4741 - val_acc: 0.8800\n",
      "Epoch 428/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1224 - acc: 0.9867 - val_loss: 0.4963 - val_acc: 0.8622\n",
      "Epoch 429/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1237 - acc: 0.9793 - val_loss: 0.4945 - val_acc: 0.8578\n",
      "Epoch 430/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1188 - acc: 0.9867 - val_loss: 0.4891 - val_acc: 0.8756\n",
      "Epoch 431/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1103 - acc: 0.9911 - val_loss: 0.4825 - val_acc: 0.8711\n",
      "Epoch 432/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1153 - acc: 0.9896 - val_loss: 0.4892 - val_acc: 0.8756\n",
      "Epoch 433/1000\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1240 - acc: 0.984 - 0s 21us/step - loss: 0.1268 - acc: 0.9896 - val_loss: 0.4785 - val_acc: 0.8800\n",
      "Epoch 434/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1230 - acc: 0.9867 - val_loss: 0.4708 - val_acc: 0.8844\n",
      "Epoch 435/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1218 - acc: 0.9763 - val_loss: 0.4734 - val_acc: 0.8800\n",
      "Epoch 436/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1100 - acc: 0.9926 - val_loss: 0.4842 - val_acc: 0.8578\n",
      "Epoch 437/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1152 - acc: 0.9896 - val_loss: 0.4790 - val_acc: 0.8800\n",
      "Epoch 438/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1213 - acc: 0.9793 - val_loss: 0.4802 - val_acc: 0.8711\n",
      "Epoch 439/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1221 - acc: 0.9867 - val_loss: 0.4978 - val_acc: 0.8489\n",
      "Epoch 440/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1233 - acc: 0.9763 - val_loss: 0.4907 - val_acc: 0.8578\n",
      "Epoch 441/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1312 - acc: 0.9763 - val_loss: 0.4837 - val_acc: 0.8622\n",
      "Epoch 442/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1142 - acc: 0.9911 - val_loss: 0.4767 - val_acc: 0.8667\n",
      "Epoch 443/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1055 - acc: 0.9911 - val_loss: 0.4681 - val_acc: 0.8756\n",
      "Epoch 444/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1177 - acc: 0.9822 - val_loss: 0.4644 - val_acc: 0.8844\n",
      "Epoch 445/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1124 - acc: 0.9881 - val_loss: 0.4712 - val_acc: 0.8622\n",
      "Epoch 446/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1143 - acc: 0.9807 - val_loss: 0.4791 - val_acc: 0.8622\n",
      "Epoch 447/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1123 - acc: 0.9867 - val_loss: 0.4727 - val_acc: 0.8800\n",
      "Epoch 448/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1106 - acc: 0.9911 - val_loss: 0.4808 - val_acc: 0.8667\n",
      "Epoch 449/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1012 - acc: 0.9970 - val_loss: 0.4922 - val_acc: 0.8622\n",
      "Epoch 450/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1067 - acc: 0.9881 - val_loss: 0.4875 - val_acc: 0.8711\n",
      "Epoch 451/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1121 - acc: 0.9941 - val_loss: 0.4877 - val_acc: 0.8711\n",
      "Epoch 452/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1097 - acc: 0.9881 - val_loss: 0.4889 - val_acc: 0.8667\n",
      "Epoch 453/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1084 - acc: 0.9881 - val_loss: 0.4882 - val_acc: 0.8711\n",
      "Epoch 454/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1123 - acc: 0.9763 - val_loss: 0.4748 - val_acc: 0.8711\n",
      "Epoch 455/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1063 - acc: 0.9911 - val_loss: 0.4890 - val_acc: 0.8667\n",
      "Epoch 456/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.1136 - acc: 0.9867 - val_loss: 0.4933 - val_acc: 0.8489\n",
      "Epoch 457/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1109 - acc: 0.9881 - val_loss: 0.4838 - val_acc: 0.8578\n",
      "Epoch 458/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0992 - acc: 0.9881 - val_loss: 0.4768 - val_acc: 0.8756\n",
      "Epoch 459/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1011 - acc: 0.9956 - val_loss: 0.4884 - val_acc: 0.8489\n",
      "Epoch 460/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1104 - acc: 0.9911 - val_loss: 0.5044 - val_acc: 0.8356\n",
      "Epoch 461/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1090 - acc: 0.9852 - val_loss: 0.4783 - val_acc: 0.8711\n",
      "Epoch 462/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1147 - acc: 0.9822 - val_loss: 0.4674 - val_acc: 0.8756\n",
      "Epoch 463/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0983 - acc: 0.9956 - val_loss: 0.4713 - val_acc: 0.8711\n",
      "Epoch 464/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1066 - acc: 0.9867 - val_loss: 0.4617 - val_acc: 0.8800\n",
      "Epoch 465/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.1096 - acc: 0.9867 - val_loss: 0.4623 - val_acc: 0.8844\n",
      "Epoch 466/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1040 - acc: 0.9896 - val_loss: 0.4645 - val_acc: 0.8756\n",
      "Epoch 467/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.1122 - acc: 0.9867 - val_loss: 0.4687 - val_acc: 0.8756\n",
      "Epoch 468/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.1107 - acc: 0.9837 - val_loss: 0.4671 - val_acc: 0.8711\n",
      "Epoch 469/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.1029 - acc: 0.9867 - val_loss: 0.4741 - val_acc: 0.8622\n",
      "Epoch 470/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0960 - acc: 0.9911 - val_loss: 0.4779 - val_acc: 0.8622\n",
      "Epoch 471/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.1079 - acc: 0.9896 - val_loss: 0.4602 - val_acc: 0.8711\n",
      "Epoch 472/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0961 - acc: 0.9896 - val_loss: 0.4503 - val_acc: 0.8933\n",
      "Epoch 473/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1001 - acc: 0.9941 - val_loss: 0.4512 - val_acc: 0.8844\n",
      "Epoch 474/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0984 - acc: 0.9926 - val_loss: 0.4533 - val_acc: 0.8711\n",
      "Epoch 475/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.1141 - acc: 0.9807 - val_loss: 0.4631 - val_acc: 0.8622\n",
      "Epoch 476/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0988 - acc: 0.9852 - val_loss: 0.4729 - val_acc: 0.8667\n",
      "Epoch 477/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1134 - acc: 0.9852 - val_loss: 0.4646 - val_acc: 0.8844\n",
      "Epoch 478/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0929 - acc: 0.9911 - val_loss: 0.4630 - val_acc: 0.8756\n",
      "Epoch 479/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0986 - acc: 0.9896 - val_loss: 0.4692 - val_acc: 0.8622\n",
      "Epoch 480/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 19us/step - loss: 0.0974 - acc: 0.9896 - val_loss: 0.4657 - val_acc: 0.8711\n",
      "Epoch 481/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.1022 - acc: 0.9896 - val_loss: 0.4647 - val_acc: 0.8711\n",
      "Epoch 482/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.1036 - acc: 0.9896 - val_loss: 0.4617 - val_acc: 0.8711\n",
      "Epoch 483/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1001 - acc: 0.9867 - val_loss: 0.4590 - val_acc: 0.8844\n",
      "Epoch 484/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0917 - acc: 0.9926 - val_loss: 0.4609 - val_acc: 0.8800\n",
      "Epoch 485/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0964 - acc: 0.9911 - val_loss: 0.4714 - val_acc: 0.8667\n",
      "Epoch 486/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0988 - acc: 0.9881 - val_loss: 0.4726 - val_acc: 0.8667\n",
      "Epoch 487/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1108 - acc: 0.9837 - val_loss: 0.4589 - val_acc: 0.8800\n",
      "Epoch 488/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1066 - acc: 0.9867 - val_loss: 0.4641 - val_acc: 0.8800\n",
      "Epoch 489/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1062 - acc: 0.9852 - val_loss: 0.4776 - val_acc: 0.8533\n",
      "Epoch 490/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0982 - acc: 0.9881 - val_loss: 0.4662 - val_acc: 0.8756\n",
      "Epoch 491/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1088 - acc: 0.9793 - val_loss: 0.4573 - val_acc: 0.8844\n",
      "Epoch 492/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0970 - acc: 0.9941 - val_loss: 0.4574 - val_acc: 0.8800\n",
      "Epoch 493/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1031 - acc: 0.9881 - val_loss: 0.4627 - val_acc: 0.8756\n",
      "Epoch 494/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0948 - acc: 0.9911 - val_loss: 0.4610 - val_acc: 0.8756\n",
      "Epoch 495/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0934 - acc: 0.9926 - val_loss: 0.4703 - val_acc: 0.8800\n",
      "Epoch 496/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0890 - acc: 0.9896 - val_loss: 0.4698 - val_acc: 0.8800\n",
      "Epoch 497/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0915 - acc: 0.9926 - val_loss: 0.4558 - val_acc: 0.8844\n",
      "Epoch 498/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0897 - acc: 0.9852 - val_loss: 0.4560 - val_acc: 0.8800\n",
      "Epoch 499/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0898 - acc: 0.9896 - val_loss: 0.4591 - val_acc: 0.8756\n",
      "Epoch 500/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0938 - acc: 0.9896 - val_loss: 0.4640 - val_acc: 0.8756\n",
      "Epoch 501/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0961 - acc: 0.9852 - val_loss: 0.4551 - val_acc: 0.8800\n",
      "Epoch 502/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1009 - acc: 0.9852 - val_loss: 0.4468 - val_acc: 0.8889\n",
      "Epoch 503/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0905 - acc: 0.9896 - val_loss: 0.4614 - val_acc: 0.8711\n",
      "Epoch 504/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0942 - acc: 0.9837 - val_loss: 0.4676 - val_acc: 0.8578\n",
      "Epoch 505/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1069 - acc: 0.9793 - val_loss: 0.4488 - val_acc: 0.8933\n",
      "Epoch 506/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0948 - acc: 0.9852 - val_loss: 0.4474 - val_acc: 0.8933\n",
      "Epoch 507/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1023 - acc: 0.9807 - val_loss: 0.4598 - val_acc: 0.8622\n",
      "Epoch 508/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0897 - acc: 0.9911 - val_loss: 0.4586 - val_acc: 0.8756\n",
      "Epoch 509/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0894 - acc: 0.9896 - val_loss: 0.4471 - val_acc: 0.8889\n",
      "Epoch 510/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0868 - acc: 0.9926 - val_loss: 0.4486 - val_acc: 0.8889\n",
      "Epoch 511/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0969 - acc: 0.9837 - val_loss: 0.4609 - val_acc: 0.8756\n",
      "Epoch 512/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0834 - acc: 0.9896 - val_loss: 0.4755 - val_acc: 0.8667\n",
      "Epoch 513/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1065 - acc: 0.9748 - val_loss: 0.4463 - val_acc: 0.8889\n",
      "Epoch 514/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0950 - acc: 0.9896 - val_loss: 0.4505 - val_acc: 0.8800\n",
      "Epoch 515/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0882 - acc: 0.9911 - val_loss: 0.4662 - val_acc: 0.8533\n",
      "Epoch 516/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0931 - acc: 0.9867 - val_loss: 0.4604 - val_acc: 0.8667\n",
      "Epoch 517/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0859 - acc: 0.9896 - val_loss: 0.4597 - val_acc: 0.8667\n",
      "Epoch 518/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.0984 - acc: 0.9881 - val_loss: 0.4546 - val_acc: 0.8844\n",
      "Epoch 519/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0810 - acc: 0.9956 - val_loss: 0.4490 - val_acc: 0.8889\n",
      "Epoch 520/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0954 - acc: 0.9911 - val_loss: 0.4415 - val_acc: 0.8889\n",
      "Epoch 521/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0903 - acc: 0.9881 - val_loss: 0.4420 - val_acc: 0.8889\n",
      "Epoch 522/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0913 - acc: 0.9852 - val_loss: 0.4496 - val_acc: 0.8800\n",
      "Epoch 523/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.0902 - acc: 0.9881 - val_loss: 0.4519 - val_acc: 0.8800\n",
      "Epoch 524/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0872 - acc: 0.9911 - val_loss: 0.4538 - val_acc: 0.8844\n",
      "Epoch 525/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0937 - acc: 0.9852 - val_loss: 0.4487 - val_acc: 0.8889\n",
      "Epoch 526/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0880 - acc: 0.9881 - val_loss: 0.4460 - val_acc: 0.8844\n",
      "Epoch 527/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0842 - acc: 0.9926 - val_loss: 0.4542 - val_acc: 0.8756\n",
      "Epoch 528/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0914 - acc: 0.9911 - val_loss: 0.4696 - val_acc: 0.8756\n",
      "Epoch 529/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0876 - acc: 0.9896 - val_loss: 0.4637 - val_acc: 0.8711\n",
      "Epoch 530/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0812 - acc: 0.9926 - val_loss: 0.4624 - val_acc: 0.8711\n",
      "Epoch 531/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.0913 - acc: 0.9852 - val_loss: 0.4632 - val_acc: 0.8756\n",
      "Epoch 532/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0870 - acc: 0.9926 - val_loss: 0.4603 - val_acc: 0.8711\n",
      "Epoch 533/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0870 - acc: 0.9881 - val_loss: 0.4581 - val_acc: 0.8711\n",
      "Epoch 534/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0981 - acc: 0.9896 - val_loss: 0.4594 - val_acc: 0.8844\n",
      "Epoch 535/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0793 - acc: 0.9926 - val_loss: 0.4484 - val_acc: 0.8800\n",
      "Epoch 536/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.0809 - acc: 0.9896 - val_loss: 0.4527 - val_acc: 0.8756\n",
      "Epoch 537/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.1006 - acc: 0.9881 - val_loss: 0.4562 - val_acc: 0.8800\n",
      "Epoch 538/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0973 - acc: 0.9867 - val_loss: 0.4516 - val_acc: 0.8800\n",
      "Epoch 539/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0834 - acc: 0.9911 - val_loss: 0.4357 - val_acc: 0.8933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 540/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0835 - acc: 0.9926 - val_loss: 0.4443 - val_acc: 0.8889\n",
      "Epoch 541/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0932 - acc: 0.9822 - val_loss: 0.4610 - val_acc: 0.8711\n",
      "Epoch 542/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0779 - acc: 0.9896 - val_loss: 0.4442 - val_acc: 0.8711\n",
      "Epoch 543/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0833 - acc: 0.9941 - val_loss: 0.4368 - val_acc: 0.8800\n",
      "Epoch 544/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.0791 - acc: 0.9896 - val_loss: 0.4436 - val_acc: 0.8667\n",
      "Epoch 545/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0912 - acc: 0.9926 - val_loss: 0.4497 - val_acc: 0.8622\n",
      "Epoch 546/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0843 - acc: 0.9896 - val_loss: 0.4491 - val_acc: 0.8578\n",
      "Epoch 547/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0818 - acc: 0.9896 - val_loss: 0.4486 - val_acc: 0.8667\n",
      "Epoch 548/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.0935 - acc: 0.9881 - val_loss: 0.4445 - val_acc: 0.8889\n",
      "Epoch 549/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0745 - acc: 0.9956 - val_loss: 0.4494 - val_acc: 0.8756\n",
      "Epoch 550/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0734 - acc: 0.9911 - val_loss: 0.4456 - val_acc: 0.8889\n",
      "Epoch 551/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0847 - acc: 0.9852 - val_loss: 0.4334 - val_acc: 0.8933\n",
      "Epoch 552/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0813 - acc: 0.9926 - val_loss: 0.4312 - val_acc: 0.8978\n",
      "Epoch 553/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0828 - acc: 0.9881 - val_loss: 0.4332 - val_acc: 0.8889\n",
      "Epoch 554/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0739 - acc: 0.9956 - val_loss: 0.4265 - val_acc: 0.8933\n",
      "Epoch 555/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0853 - acc: 0.9881 - val_loss: 0.4348 - val_acc: 0.8889\n",
      "Epoch 556/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.0826 - acc: 0.9852 - val_loss: 0.4516 - val_acc: 0.8889\n",
      "Epoch 557/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0748 - acc: 0.9941 - val_loss: 0.4386 - val_acc: 0.8800\n",
      "Epoch 558/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0865 - acc: 0.9896 - val_loss: 0.4377 - val_acc: 0.8844\n",
      "Epoch 559/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0746 - acc: 0.9896 - val_loss: 0.4580 - val_acc: 0.8667\n",
      "Epoch 560/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0859 - acc: 0.9867 - val_loss: 0.4592 - val_acc: 0.8622\n",
      "Epoch 561/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0732 - acc: 0.9926 - val_loss: 0.4429 - val_acc: 0.8800\n",
      "Epoch 562/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0793 - acc: 0.9956 - val_loss: 0.4336 - val_acc: 0.8800\n",
      "Epoch 563/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0834 - acc: 0.9852 - val_loss: 0.4429 - val_acc: 0.8844\n",
      "Epoch 564/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0838 - acc: 0.9896 - val_loss: 0.4543 - val_acc: 0.8800\n",
      "Epoch 565/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0785 - acc: 0.9911 - val_loss: 0.4464 - val_acc: 0.8756\n",
      "Epoch 566/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0774 - acc: 0.9911 - val_loss: 0.4329 - val_acc: 0.8978\n",
      "Epoch 567/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.1019 - acc: 0.9748 - val_loss: 0.4374 - val_acc: 0.8756\n",
      "Epoch 568/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0759 - acc: 0.9941 - val_loss: 0.4426 - val_acc: 0.8756\n",
      "Epoch 569/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0764 - acc: 0.9896 - val_loss: 0.4499 - val_acc: 0.8800\n",
      "Epoch 570/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0771 - acc: 0.9881 - val_loss: 0.4553 - val_acc: 0.8800\n",
      "Epoch 571/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0792 - acc: 0.9881 - val_loss: 0.4492 - val_acc: 0.8800\n",
      "Epoch 572/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0783 - acc: 0.9926 - val_loss: 0.4470 - val_acc: 0.8800\n",
      "Epoch 573/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.0781 - acc: 0.9941 - val_loss: 0.4598 - val_acc: 0.8667\n",
      "Epoch 574/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0749 - acc: 0.9896 - val_loss: 0.4547 - val_acc: 0.8800\n",
      "Epoch 575/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0749 - acc: 0.9911 - val_loss: 0.4473 - val_acc: 0.8756\n",
      "Epoch 576/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0759 - acc: 0.9926 - val_loss: 0.4566 - val_acc: 0.8711\n",
      "Epoch 577/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0662 - acc: 0.9956 - val_loss: 0.4774 - val_acc: 0.8578\n",
      "Epoch 578/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0754 - acc: 0.9896 - val_loss: 0.4622 - val_acc: 0.8578\n",
      "Epoch 579/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0754 - acc: 0.9881 - val_loss: 0.4431 - val_acc: 0.8800\n",
      "Epoch 580/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0644 - acc: 0.9956 - val_loss: 0.4415 - val_acc: 0.8756\n",
      "Epoch 581/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0865 - acc: 0.9867 - val_loss: 0.4503 - val_acc: 0.8800\n",
      "Epoch 582/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0789 - acc: 0.9941 - val_loss: 0.4413 - val_acc: 0.8844\n",
      "Epoch 583/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0769 - acc: 0.9881 - val_loss: 0.4298 - val_acc: 0.8844\n",
      "Epoch 584/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0671 - acc: 0.9941 - val_loss: 0.4272 - val_acc: 0.8844\n",
      "Epoch 585/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0724 - acc: 0.9941 - val_loss: 0.4307 - val_acc: 0.8844\n",
      "Epoch 586/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0853 - acc: 0.9852 - val_loss: 0.4343 - val_acc: 0.8844\n",
      "Epoch 587/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0636 - acc: 0.9970 - val_loss: 0.4419 - val_acc: 0.8800\n",
      "Epoch 588/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.0718 - acc: 0.9941 - val_loss: 0.4565 - val_acc: 0.8578\n",
      "Epoch 589/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0732 - acc: 0.9911 - val_loss: 0.4504 - val_acc: 0.8622\n",
      "Epoch 590/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0732 - acc: 0.9896 - val_loss: 0.4344 - val_acc: 0.8844\n",
      "Epoch 591/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0666 - acc: 0.9956 - val_loss: 0.4280 - val_acc: 0.8889\n",
      "Epoch 592/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0750 - acc: 0.9941 - val_loss: 0.4336 - val_acc: 0.8844\n",
      "Epoch 593/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0749 - acc: 0.9911 - val_loss: 0.4467 - val_acc: 0.8756\n",
      "Epoch 594/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0664 - acc: 0.9956 - val_loss: 0.4443 - val_acc: 0.8844\n",
      "Epoch 595/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0781 - acc: 0.9867 - val_loss: 0.4320 - val_acc: 0.8933\n",
      "Epoch 596/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0658 - acc: 0.9911 - val_loss: 0.4232 - val_acc: 0.8889\n",
      "Epoch 597/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0680 - acc: 0.9926 - val_loss: 0.4278 - val_acc: 0.8933\n",
      "Epoch 598/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.0694 - acc: 0.9926 - val_loss: 0.4420 - val_acc: 0.8622\n",
      "Epoch 599/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.0733 - acc: 0.9926 - val_loss: 0.4230 - val_acc: 0.8933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0743 - acc: 0.9926 - val_loss: 0.4260 - val_acc: 0.8933\n",
      "Epoch 601/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.0701 - acc: 0.9896 - val_loss: 0.4458 - val_acc: 0.8844\n",
      "Epoch 602/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0727 - acc: 0.9911 - val_loss: 0.4310 - val_acc: 0.8889\n",
      "Epoch 603/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0613 - acc: 0.9970 - val_loss: 0.4214 - val_acc: 0.8756\n",
      "Epoch 604/1000\n",
      "675/675 [==============================] - 0s 19us/step - loss: 0.0791 - acc: 0.9926 - val_loss: 0.4220 - val_acc: 0.8844\n",
      "Epoch 605/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0621 - acc: 0.9911 - val_loss: 0.4307 - val_acc: 0.8844\n",
      "Epoch 606/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0802 - acc: 0.9881 - val_loss: 0.4155 - val_acc: 0.8889\n",
      "Epoch 607/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0718 - acc: 0.9956 - val_loss: 0.4197 - val_acc: 0.8889\n",
      "Epoch 608/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0746 - acc: 0.9867 - val_loss: 0.4327 - val_acc: 0.8711\n",
      "Epoch 609/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0746 - acc: 0.9881 - val_loss: 0.4293 - val_acc: 0.8844\n",
      "Epoch 610/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0735 - acc: 0.9881 - val_loss: 0.4224 - val_acc: 0.8889\n",
      "Epoch 611/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0697 - acc: 0.9941 - val_loss: 0.4213 - val_acc: 0.8889\n",
      "Epoch 612/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0737 - acc: 0.9911 - val_loss: 0.4272 - val_acc: 0.8889\n",
      "Epoch 613/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0679 - acc: 0.9867 - val_loss: 0.4264 - val_acc: 0.8889\n",
      "Epoch 614/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0726 - acc: 0.9926 - val_loss: 0.4409 - val_acc: 0.8844\n",
      "Epoch 615/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0740 - acc: 0.9867 - val_loss: 0.4444 - val_acc: 0.8889\n",
      "Epoch 616/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0744 - acc: 0.9911 - val_loss: 0.4268 - val_acc: 0.8933\n",
      "Epoch 617/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0770 - acc: 0.9911 - val_loss: 0.4206 - val_acc: 0.8978\n",
      "Epoch 618/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0713 - acc: 0.9881 - val_loss: 0.4433 - val_acc: 0.8711\n",
      "Epoch 619/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.0809 - acc: 0.9881 - val_loss: 0.4529 - val_acc: 0.8711\n",
      "Epoch 620/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0656 - acc: 0.9970 - val_loss: 0.4398 - val_acc: 0.8844\n",
      "Epoch 621/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0727 - acc: 0.9881 - val_loss: 0.4292 - val_acc: 0.9022\n",
      "Epoch 622/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0693 - acc: 0.9881 - val_loss: 0.4438 - val_acc: 0.8800\n",
      "Epoch 623/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0775 - acc: 0.9867 - val_loss: 0.4634 - val_acc: 0.8533\n",
      "Epoch 624/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.0717 - acc: 0.9896 - val_loss: 0.4462 - val_acc: 0.8756\n",
      "Epoch 625/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0657 - acc: 0.9941 - val_loss: 0.4238 - val_acc: 0.9067\n",
      "Epoch 626/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0783 - acc: 0.9867 - val_loss: 0.4288 - val_acc: 0.9022\n",
      "Epoch 627/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0687 - acc: 0.9911 - val_loss: 0.4526 - val_acc: 0.8756\n",
      "Epoch 628/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0680 - acc: 0.9911 - val_loss: 0.4336 - val_acc: 0.8889\n",
      "Epoch 629/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0617 - acc: 0.9896 - val_loss: 0.4268 - val_acc: 0.8800\n",
      "Epoch 630/1000\n",
      "675/675 [==============================] - 0s 20us/step - loss: 0.0724 - acc: 0.9881 - val_loss: 0.4322 - val_acc: 0.8756\n",
      "Epoch 631/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0629 - acc: 0.9911 - val_loss: 0.4347 - val_acc: 0.8800\n",
      "Epoch 632/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0691 - acc: 0.9896 - val_loss: 0.4455 - val_acc: 0.8800\n",
      "Epoch 633/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0668 - acc: 0.9970 - val_loss: 0.4418 - val_acc: 0.8800\n",
      "Epoch 634/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.0764 - acc: 0.9822 - val_loss: 0.4211 - val_acc: 0.8978\n",
      "Epoch 635/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 0.0668 - acc: 0.9911 - val_loss: 0.4173 - val_acc: 0.8978\n",
      "Epoch 636/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.0721 - acc: 0.9926 - val_loss: 0.4432 - val_acc: 0.8800\n",
      "Epoch 637/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.0591 - acc: 0.9956 - val_loss: 0.4505 - val_acc: 0.8667\n",
      "Epoch 638/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.0697 - acc: 0.9881 - val_loss: 0.4317 - val_acc: 0.8844\n",
      "Epoch 639/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.0608 - acc: 0.9941 - val_loss: 0.4214 - val_acc: 0.8889\n",
      "Epoch 640/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.0560 - acc: 1.0000 - val_loss: 0.4228 - val_acc: 0.8889\n",
      "Epoch 641/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.0641 - acc: 0.9896 - val_loss: 0.4462 - val_acc: 0.8889\n",
      "Epoch 642/1000\n",
      "675/675 [==============================] - 0s 39us/step - loss: 0.0605 - acc: 0.9926 - val_loss: 0.4489 - val_acc: 0.8800\n",
      "Epoch 643/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 0.0641 - acc: 0.9941 - val_loss: 0.4326 - val_acc: 0.8844\n",
      "Epoch 644/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.0659 - acc: 0.9941 - val_loss: 0.4279 - val_acc: 0.8844\n",
      "Epoch 645/1000\n",
      "675/675 [==============================] - 0s 34us/step - loss: 0.0632 - acc: 0.9867 - val_loss: 0.4354 - val_acc: 0.8800\n",
      "Epoch 646/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0687 - acc: 0.9911 - val_loss: 0.4469 - val_acc: 0.8711\n",
      "Epoch 647/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.0743 - acc: 0.9881 - val_loss: 0.4400 - val_acc: 0.8844\n",
      "Epoch 648/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0715 - acc: 0.9867 - val_loss: 0.4304 - val_acc: 0.8844\n",
      "Epoch 649/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.0717 - acc: 0.9911 - val_loss: 0.4379 - val_acc: 0.8711\n",
      "Epoch 650/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.0686 - acc: 0.9896 - val_loss: 0.4263 - val_acc: 0.8889\n",
      "Epoch 651/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0683 - acc: 0.9896 - val_loss: 0.4241 - val_acc: 0.8978\n",
      "Epoch 652/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0660 - acc: 0.9956 - val_loss: 0.4280 - val_acc: 0.8889\n",
      "Epoch 653/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0692 - acc: 0.9881 - val_loss: 0.4243 - val_acc: 0.8933\n",
      "Epoch 654/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0614 - acc: 0.9956 - val_loss: 0.4138 - val_acc: 0.8889\n",
      "Epoch 655/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0605 - acc: 0.9941 - val_loss: 0.4205 - val_acc: 0.8889\n",
      "Epoch 656/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0558 - acc: 0.9956 - val_loss: 0.4172 - val_acc: 0.8889\n",
      "Epoch 657/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.0603 - acc: 0.9941 - val_loss: 0.4216 - val_acc: 0.8933\n",
      "Epoch 658/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0617 - acc: 0.9941 - val_loss: 0.4199 - val_acc: 0.8844\n",
      "Epoch 659/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.0580 - acc: 0.9881 - val_loss: 0.4200 - val_acc: 0.8933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 660/1000\n",
      "675/675 [==============================] - 0s 33us/step - loss: 0.0541 - acc: 0.9956 - val_loss: 0.4291 - val_acc: 0.8756\n",
      "Epoch 661/1000\n",
      "675/675 [==============================] - 0s 31us/step - loss: 0.0689 - acc: 0.9911 - val_loss: 0.4199 - val_acc: 0.8889\n",
      "Epoch 662/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.0586 - acc: 0.9941 - val_loss: 0.4335 - val_acc: 0.8756\n",
      "Epoch 663/1000\n",
      "675/675 [==============================] - 0s 29us/step - loss: 0.0596 - acc: 0.9941 - val_loss: 0.4479 - val_acc: 0.8756\n",
      "Epoch 664/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.0697 - acc: 0.9881 - val_loss: 0.4359 - val_acc: 0.8889\n",
      "Epoch 665/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0589 - acc: 0.9956 - val_loss: 0.4322 - val_acc: 0.8933\n",
      "Epoch 666/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0602 - acc: 0.9896 - val_loss: 0.4380 - val_acc: 0.8889\n",
      "Epoch 667/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.0614 - acc: 0.9941 - val_loss: 0.4294 - val_acc: 0.8933\n",
      "Epoch 668/1000\n",
      "675/675 [==============================] - 0s 38us/step - loss: 0.0670 - acc: 0.9881 - val_loss: 0.4315 - val_acc: 0.8933\n",
      "Epoch 669/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.0685 - acc: 0.9911 - val_loss: 0.4313 - val_acc: 0.8933\n",
      "Epoch 670/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0616 - acc: 0.9941 - val_loss: 0.4416 - val_acc: 0.8800\n",
      "Epoch 671/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.0595 - acc: 0.9956 - val_loss: 0.4319 - val_acc: 0.8889\n",
      "Epoch 672/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0630 - acc: 0.9926 - val_loss: 0.4226 - val_acc: 0.8933\n",
      "Epoch 673/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.0594 - acc: 0.9911 - val_loss: 0.4310 - val_acc: 0.8889\n",
      "Epoch 674/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.0678 - acc: 0.9911 - val_loss: 0.4339 - val_acc: 0.8844\n",
      "Epoch 675/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.0605 - acc: 0.9956 - val_loss: 0.4263 - val_acc: 0.8933\n",
      "Epoch 676/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0608 - acc: 0.9956 - val_loss: 0.4395 - val_acc: 0.8800\n",
      "Epoch 677/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0624 - acc: 0.9911 - val_loss: 0.4323 - val_acc: 0.8889\n",
      "Epoch 678/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0615 - acc: 0.9896 - val_loss: 0.4206 - val_acc: 0.8978\n",
      "Epoch 679/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.0559 - acc: 0.9970 - val_loss: 0.4180 - val_acc: 0.8978\n",
      "Epoch 680/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0621 - acc: 0.9867 - val_loss: 0.4268 - val_acc: 0.8933\n",
      "Epoch 681/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0678 - acc: 0.9881 - val_loss: 0.4359 - val_acc: 0.8800\n",
      "Epoch 682/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.0630 - acc: 0.9926 - val_loss: 0.4366 - val_acc: 0.8800\n",
      "Epoch 683/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.0542 - acc: 0.9985 - val_loss: 0.4218 - val_acc: 0.8889\n",
      "Epoch 684/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.0565 - acc: 0.9956 - val_loss: 0.4348 - val_acc: 0.8711\n",
      "Epoch 685/1000\n",
      "675/675 [==============================] - 0s 30us/step - loss: 0.0589 - acc: 0.9926 - val_loss: 0.4534 - val_acc: 0.8489\n",
      "Epoch 686/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.0702 - acc: 0.9852 - val_loss: 0.4281 - val_acc: 0.8844\n",
      "Epoch 687/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.0605 - acc: 0.9941 - val_loss: 0.4293 - val_acc: 0.8844\n",
      "Epoch 688/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0646 - acc: 0.9881 - val_loss: 0.4419 - val_acc: 0.8711\n",
      "Epoch 689/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0577 - acc: 0.9926 - val_loss: 0.4387 - val_acc: 0.8800\n",
      "Epoch 690/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0584 - acc: 0.9896 - val_loss: 0.4445 - val_acc: 0.8756\n",
      "Epoch 691/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0551 - acc: 0.9926 - val_loss: 0.4467 - val_acc: 0.8711\n",
      "Epoch 692/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0633 - acc: 0.9911 - val_loss: 0.4404 - val_acc: 0.8756\n",
      "Epoch 693/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0609 - acc: 0.9896 - val_loss: 0.4239 - val_acc: 0.8889\n",
      "Epoch 694/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0626 - acc: 0.9896 - val_loss: 0.4380 - val_acc: 0.8889\n",
      "Epoch 695/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0473 - acc: 0.9956 - val_loss: 0.4495 - val_acc: 0.8844\n",
      "Epoch 696/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0563 - acc: 0.9956 - val_loss: 0.4326 - val_acc: 0.8933\n",
      "Epoch 697/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0538 - acc: 0.9926 - val_loss: 0.4252 - val_acc: 0.8978\n",
      "Epoch 698/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0631 - acc: 0.9926 - val_loss: 0.4289 - val_acc: 0.8933\n",
      "Epoch 699/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0599 - acc: 0.9926 - val_loss: 0.4438 - val_acc: 0.8844\n",
      "Epoch 700/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0524 - acc: 0.9926 - val_loss: 0.4415 - val_acc: 0.8800\n",
      "Epoch 701/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0605 - acc: 0.9911 - val_loss: 0.4206 - val_acc: 0.8889\n",
      "Epoch 702/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0561 - acc: 0.9941 - val_loss: 0.4099 - val_acc: 0.8933\n",
      "Epoch 703/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0692 - acc: 0.9881 - val_loss: 0.4132 - val_acc: 0.8889\n",
      "Epoch 704/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0504 - acc: 0.9970 - val_loss: 0.4282 - val_acc: 0.8844\n",
      "Epoch 705/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0517 - acc: 0.9970 - val_loss: 0.4472 - val_acc: 0.8667\n",
      "Epoch 706/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0661 - acc: 0.9852 - val_loss: 0.4316 - val_acc: 0.8756\n",
      "Epoch 707/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0590 - acc: 0.9896 - val_loss: 0.4116 - val_acc: 0.8933\n",
      "Epoch 708/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0560 - acc: 0.9926 - val_loss: 0.4086 - val_acc: 0.8933\n",
      "Epoch 709/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0572 - acc: 0.9941 - val_loss: 0.4218 - val_acc: 0.8933\n",
      "Epoch 710/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0573 - acc: 0.9926 - val_loss: 0.4128 - val_acc: 0.8933\n",
      "Epoch 711/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0596 - acc: 0.9896 - val_loss: 0.4022 - val_acc: 0.9022\n",
      "Epoch 712/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0494 - acc: 0.9985 - val_loss: 0.4050 - val_acc: 0.9022\n",
      "Epoch 713/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0625 - acc: 0.9926 - val_loss: 0.4153 - val_acc: 0.8889\n",
      "Epoch 714/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.0565 - acc: 0.9911 - val_loss: 0.4227 - val_acc: 0.8667\n",
      "Epoch 715/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0623 - acc: 0.9852 - val_loss: 0.4333 - val_acc: 0.8711\n",
      "Epoch 716/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0525 - acc: 0.9911 - val_loss: 0.4354 - val_acc: 0.8800\n",
      "Epoch 717/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0593 - acc: 0.9926 - val_loss: 0.4564 - val_acc: 0.8667\n",
      "Epoch 718/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0557 - acc: 0.9911 - val_loss: 0.4674 - val_acc: 0.8622\n",
      "Epoch 719/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0599 - acc: 0.9911 - val_loss: 0.4490 - val_acc: 0.8622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 720/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0568 - acc: 0.9941 - val_loss: 0.4366 - val_acc: 0.8844\n",
      "Epoch 721/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0585 - acc: 0.9926 - val_loss: 0.4313 - val_acc: 0.8889\n",
      "Epoch 722/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0480 - acc: 0.9956 - val_loss: 0.4339 - val_acc: 0.8933\n",
      "Epoch 723/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0455 - acc: 0.9985 - val_loss: 0.4332 - val_acc: 0.8933\n",
      "Epoch 724/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0658 - acc: 0.9896 - val_loss: 0.4257 - val_acc: 0.8978\n",
      "Epoch 725/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0500 - acc: 0.9970 - val_loss: 0.4304 - val_acc: 0.8933\n",
      "Epoch 726/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0671 - acc: 0.9867 - val_loss: 0.4426 - val_acc: 0.8711\n",
      "Epoch 727/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0637 - acc: 0.9896 - val_loss: 0.4354 - val_acc: 0.8933\n",
      "Epoch 728/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0527 - acc: 0.9941 - val_loss: 0.4339 - val_acc: 0.8889\n",
      "Epoch 729/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0562 - acc: 0.9941 - val_loss: 0.4602 - val_acc: 0.8711\n",
      "Epoch 730/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0541 - acc: 0.9911 - val_loss: 0.4469 - val_acc: 0.8800\n",
      "Epoch 731/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0614 - acc: 0.9896 - val_loss: 0.4305 - val_acc: 0.8800\n",
      "Epoch 732/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0551 - acc: 0.9941 - val_loss: 0.4322 - val_acc: 0.8800\n",
      "Epoch 733/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0631 - acc: 0.9896 - val_loss: 0.4406 - val_acc: 0.8756\n",
      "Epoch 734/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0569 - acc: 0.9896 - val_loss: 0.4391 - val_acc: 0.8800\n",
      "Epoch 735/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0565 - acc: 0.9956 - val_loss: 0.4216 - val_acc: 0.8844\n",
      "Epoch 736/1000\n",
      "675/675 [==============================] - 0s 28us/step - loss: 0.0552 - acc: 0.9896 - val_loss: 0.4236 - val_acc: 0.8844\n",
      "Epoch 737/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0591 - acc: 0.9926 - val_loss: 0.4341 - val_acc: 0.8800\n",
      "Epoch 738/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0611 - acc: 0.9881 - val_loss: 0.4396 - val_acc: 0.8800\n",
      "Epoch 739/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0585 - acc: 0.9881 - val_loss: 0.4377 - val_acc: 0.8756\n",
      "Epoch 740/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0593 - acc: 0.9896 - val_loss: 0.4345 - val_acc: 0.8622\n",
      "Epoch 741/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0488 - acc: 0.9941 - val_loss: 0.4275 - val_acc: 0.8756\n",
      "Epoch 742/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0500 - acc: 0.9941 - val_loss: 0.4352 - val_acc: 0.8667\n",
      "Epoch 743/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0500 - acc: 0.9941 - val_loss: 0.4358 - val_acc: 0.8667\n",
      "Epoch 744/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0502 - acc: 0.9941 - val_loss: 0.4249 - val_acc: 0.8711\n",
      "Epoch 745/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0588 - acc: 0.9911 - val_loss: 0.4280 - val_acc: 0.8889\n",
      "Epoch 746/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0514 - acc: 0.9926 - val_loss: 0.4423 - val_acc: 0.8756\n",
      "Epoch 747/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0476 - acc: 0.9956 - val_loss: 0.4283 - val_acc: 0.8889\n",
      "Epoch 748/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0618 - acc: 0.9867 - val_loss: 0.4118 - val_acc: 0.8844\n",
      "Epoch 749/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0567 - acc: 0.9926 - val_loss: 0.4221 - val_acc: 0.8844\n",
      "Epoch 750/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0602 - acc: 0.9881 - val_loss: 0.4503 - val_acc: 0.8844\n",
      "Epoch 751/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0602 - acc: 0.9881 - val_loss: 0.4287 - val_acc: 0.8933\n",
      "Epoch 752/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0516 - acc: 0.9926 - val_loss: 0.4127 - val_acc: 0.8978\n",
      "Epoch 753/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0489 - acc: 0.9956 - val_loss: 0.4173 - val_acc: 0.8978\n",
      "Epoch 754/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0535 - acc: 0.9896 - val_loss: 0.4305 - val_acc: 0.8711\n",
      "Epoch 755/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0452 - acc: 0.9956 - val_loss: 0.4458 - val_acc: 0.8622\n",
      "Epoch 756/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0498 - acc: 0.9926 - val_loss: 0.4428 - val_acc: 0.8622\n",
      "Epoch 757/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0459 - acc: 0.9970 - val_loss: 0.4287 - val_acc: 0.8756\n",
      "Epoch 758/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0474 - acc: 0.9970 - val_loss: 0.4295 - val_acc: 0.8800\n",
      "Epoch 759/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0636 - acc: 0.9867 - val_loss: 0.4446 - val_acc: 0.8667\n",
      "Epoch 760/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0464 - acc: 0.9941 - val_loss: 0.4538 - val_acc: 0.8711\n",
      "Epoch 761/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0506 - acc: 0.9956 - val_loss: 0.4438 - val_acc: 0.8711\n",
      "Epoch 762/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0552 - acc: 0.9911 - val_loss: 0.4488 - val_acc: 0.8667\n",
      "Epoch 763/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0513 - acc: 0.9911 - val_loss: 0.4451 - val_acc: 0.8667\n",
      "Epoch 764/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0592 - acc: 0.9881 - val_loss: 0.4274 - val_acc: 0.8933\n",
      "Epoch 765/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0508 - acc: 0.9956 - val_loss: 0.4136 - val_acc: 0.8978\n",
      "Epoch 766/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0494 - acc: 0.9956 - val_loss: 0.4123 - val_acc: 0.8978\n",
      "Epoch 767/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0498 - acc: 0.9956 - val_loss: 0.4172 - val_acc: 0.8933\n",
      "Epoch 768/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0549 - acc: 0.9896 - val_loss: 0.4098 - val_acc: 0.8933\n",
      "Epoch 769/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0541 - acc: 0.9911 - val_loss: 0.4076 - val_acc: 0.8978\n",
      "Epoch 770/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0496 - acc: 0.9941 - val_loss: 0.4243 - val_acc: 0.8889\n",
      "Epoch 771/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0461 - acc: 0.9926 - val_loss: 0.4294 - val_acc: 0.8844\n",
      "Epoch 772/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0429 - acc: 0.9941 - val_loss: 0.4218 - val_acc: 0.8800\n",
      "Epoch 773/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0540 - acc: 0.9881 - val_loss: 0.4215 - val_acc: 0.8800\n",
      "Epoch 774/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0512 - acc: 0.9926 - val_loss: 0.4304 - val_acc: 0.8756\n",
      "Epoch 775/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0604 - acc: 0.9881 - val_loss: 0.4422 - val_acc: 0.8622\n",
      "Epoch 776/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0461 - acc: 0.9911 - val_loss: 0.4372 - val_acc: 0.8800\n",
      "Epoch 777/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0557 - acc: 0.9911 - val_loss: 0.4342 - val_acc: 0.8800\n",
      "Epoch 778/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0548 - acc: 0.9941 - val_loss: 0.4320 - val_acc: 0.8800\n",
      "Epoch 779/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0492 - acc: 0.9911 - val_loss: 0.4159 - val_acc: 0.8889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 780/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0557 - acc: 0.9926 - val_loss: 0.4132 - val_acc: 0.8933\n",
      "Epoch 781/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0518 - acc: 0.9896 - val_loss: 0.4197 - val_acc: 0.8933\n",
      "Epoch 782/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0539 - acc: 0.9926 - val_loss: 0.4187 - val_acc: 0.8889\n",
      "Epoch 783/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0545 - acc: 0.9926 - val_loss: 0.4313 - val_acc: 0.8844\n",
      "Epoch 784/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0511 - acc: 0.9867 - val_loss: 0.4428 - val_acc: 0.8711\n",
      "Epoch 785/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0527 - acc: 0.9881 - val_loss: 0.4268 - val_acc: 0.8800\n",
      "Epoch 786/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0516 - acc: 0.9926 - val_loss: 0.4228 - val_acc: 0.8756\n",
      "Epoch 787/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0492 - acc: 0.9941 - val_loss: 0.4401 - val_acc: 0.8667\n",
      "Epoch 788/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0472 - acc: 0.9911 - val_loss: 0.4386 - val_acc: 0.8667\n",
      "Epoch 789/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0460 - acc: 0.9941 - val_loss: 0.4258 - val_acc: 0.8844\n",
      "Epoch 790/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0491 - acc: 0.9867 - val_loss: 0.4215 - val_acc: 0.8844\n",
      "Epoch 791/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0492 - acc: 0.9956 - val_loss: 0.4163 - val_acc: 0.8844\n",
      "Epoch 792/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0498 - acc: 0.9941 - val_loss: 0.4251 - val_acc: 0.8844\n",
      "Epoch 793/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0530 - acc: 0.9867 - val_loss: 0.4174 - val_acc: 0.8889\n",
      "Epoch 794/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0555 - acc: 0.9941 - val_loss: 0.4198 - val_acc: 0.8889\n",
      "Epoch 795/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0480 - acc: 0.9941 - val_loss: 0.4142 - val_acc: 0.8844\n",
      "Epoch 796/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0481 - acc: 0.9926 - val_loss: 0.4132 - val_acc: 0.8889\n",
      "Epoch 797/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0493 - acc: 0.9896 - val_loss: 0.4149 - val_acc: 0.8889\n",
      "Epoch 798/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0530 - acc: 0.9896 - val_loss: 0.4114 - val_acc: 0.8844\n",
      "Epoch 799/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0415 - acc: 0.9956 - val_loss: 0.3987 - val_acc: 0.8800\n",
      "Epoch 800/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0432 - acc: 0.9970 - val_loss: 0.3945 - val_acc: 0.9022\n",
      "Epoch 801/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0621 - acc: 0.9822 - val_loss: 0.3992 - val_acc: 0.8933\n",
      "Epoch 802/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0471 - acc: 0.9941 - val_loss: 0.4238 - val_acc: 0.8844\n",
      "Epoch 803/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0390 - acc: 0.9970 - val_loss: 0.4381 - val_acc: 0.8711\n",
      "Epoch 804/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0550 - acc: 0.9941 - val_loss: 0.4160 - val_acc: 0.8844\n",
      "Epoch 805/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0430 - acc: 0.9941 - val_loss: 0.4137 - val_acc: 0.8844\n",
      "Epoch 806/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0464 - acc: 0.9941 - val_loss: 0.4263 - val_acc: 0.8800\n",
      "Epoch 807/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0521 - acc: 0.9896 - val_loss: 0.4381 - val_acc: 0.8711\n",
      "Epoch 808/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0469 - acc: 0.9911 - val_loss: 0.4228 - val_acc: 0.8844\n",
      "Epoch 809/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0425 - acc: 0.9926 - val_loss: 0.4087 - val_acc: 0.8844\n",
      "Epoch 810/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0468 - acc: 0.9941 - val_loss: 0.4216 - val_acc: 0.8844\n",
      "Epoch 811/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0525 - acc: 0.9926 - val_loss: 0.4346 - val_acc: 0.8800\n",
      "Epoch 812/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0434 - acc: 0.9956 - val_loss: 0.4278 - val_acc: 0.8844\n",
      "Epoch 813/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0471 - acc: 0.9896 - val_loss: 0.4118 - val_acc: 0.8978\n",
      "Epoch 814/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0510 - acc: 0.9926 - val_loss: 0.4071 - val_acc: 0.8933\n",
      "Epoch 815/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0460 - acc: 0.9941 - val_loss: 0.4298 - val_acc: 0.8711\n",
      "Epoch 816/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0444 - acc: 0.9970 - val_loss: 0.4312 - val_acc: 0.8711\n",
      "Epoch 817/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0447 - acc: 0.9956 - val_loss: 0.4064 - val_acc: 0.8800\n",
      "Epoch 818/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0475 - acc: 0.9956 - val_loss: 0.3977 - val_acc: 0.8844\n",
      "Epoch 819/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0474 - acc: 0.9956 - val_loss: 0.4065 - val_acc: 0.8800\n",
      "Epoch 820/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0413 - acc: 0.9926 - val_loss: 0.4220 - val_acc: 0.8667\n",
      "Epoch 821/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0422 - acc: 0.9970 - val_loss: 0.4168 - val_acc: 0.8667\n",
      "Epoch 822/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0591 - acc: 0.9867 - val_loss: 0.4090 - val_acc: 0.8800\n",
      "Epoch 823/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0518 - acc: 0.9941 - val_loss: 0.4138 - val_acc: 0.8800\n",
      "Epoch 824/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0448 - acc: 0.9970 - val_loss: 0.4466 - val_acc: 0.8622\n",
      "Epoch 825/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0583 - acc: 0.9822 - val_loss: 0.4255 - val_acc: 0.8756\n",
      "Epoch 826/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0631 - acc: 0.9852 - val_loss: 0.4256 - val_acc: 0.8667\n",
      "Epoch 827/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0467 - acc: 0.9926 - val_loss: 0.4430 - val_acc: 0.8622\n",
      "Epoch 828/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0485 - acc: 0.9926 - val_loss: 0.4573 - val_acc: 0.8622\n",
      "Epoch 829/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0434 - acc: 0.9956 - val_loss: 0.4539 - val_acc: 0.8711\n",
      "Epoch 830/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0510 - acc: 0.9926 - val_loss: 0.4395 - val_acc: 0.8711\n",
      "Epoch 831/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0537 - acc: 0.9867 - val_loss: 0.4331 - val_acc: 0.8711\n",
      "Epoch 832/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0449 - acc: 0.9926 - val_loss: 0.4282 - val_acc: 0.8844\n",
      "Epoch 833/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0487 - acc: 0.9956 - val_loss: 0.4369 - val_acc: 0.8844\n",
      "Epoch 834/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0513 - acc: 0.9911 - val_loss: 0.4276 - val_acc: 0.8889\n",
      "Epoch 835/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0430 - acc: 0.9941 - val_loss: 0.4216 - val_acc: 0.8889\n",
      "Epoch 836/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0515 - acc: 0.9911 - val_loss: 0.4130 - val_acc: 0.8889\n",
      "Epoch 837/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0413 - acc: 0.9985 - val_loss: 0.4034 - val_acc: 0.8889\n",
      "Epoch 838/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0409 - acc: 0.9956 - val_loss: 0.4271 - val_acc: 0.8711\n",
      "Epoch 839/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0482 - acc: 0.9896 - val_loss: 0.4349 - val_acc: 0.8756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 840/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0436 - acc: 0.9941 - val_loss: 0.4310 - val_acc: 0.8756\n",
      "Epoch 841/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0472 - acc: 0.9911 - val_loss: 0.4003 - val_acc: 0.9067\n",
      "Epoch 842/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0388 - acc: 0.9970 - val_loss: 0.3983 - val_acc: 0.9067\n",
      "Epoch 843/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0498 - acc: 0.9896 - val_loss: 0.4132 - val_acc: 0.8933\n",
      "Epoch 844/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0401 - acc: 0.9985 - val_loss: 0.4151 - val_acc: 0.8933\n",
      "Epoch 845/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0452 - acc: 0.9896 - val_loss: 0.3952 - val_acc: 0.9067\n",
      "Epoch 846/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0433 - acc: 0.9896 - val_loss: 0.3973 - val_acc: 0.9022\n",
      "Epoch 847/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0349 - acc: 0.9970 - val_loss: 0.4195 - val_acc: 0.8889\n",
      "Epoch 848/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0406 - acc: 0.9941 - val_loss: 0.4210 - val_acc: 0.8756\n",
      "Epoch 849/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0455 - acc: 0.9911 - val_loss: 0.4048 - val_acc: 0.8933\n",
      "Epoch 850/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0416 - acc: 0.9926 - val_loss: 0.4042 - val_acc: 0.8889\n",
      "Epoch 851/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0355 - acc: 1.0000 - val_loss: 0.4248 - val_acc: 0.8667\n",
      "Epoch 852/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0422 - acc: 0.9941 - val_loss: 0.4333 - val_acc: 0.8711\n",
      "Epoch 853/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0461 - acc: 0.9926 - val_loss: 0.4346 - val_acc: 0.8711\n",
      "Epoch 854/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0488 - acc: 0.9911 - val_loss: 0.4195 - val_acc: 0.8756\n",
      "Epoch 855/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0426 - acc: 0.9970 - val_loss: 0.4345 - val_acc: 0.8756\n",
      "Epoch 856/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0503 - acc: 0.9881 - val_loss: 0.4297 - val_acc: 0.8756\n",
      "Epoch 857/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0458 - acc: 0.9956 - val_loss: 0.3954 - val_acc: 0.8978\n",
      "Epoch 858/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0531 - acc: 0.9896 - val_loss: 0.3880 - val_acc: 0.9022\n",
      "Epoch 859/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0436 - acc: 0.9896 - val_loss: 0.4009 - val_acc: 0.8933\n",
      "Epoch 860/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0494 - acc: 0.9896 - val_loss: 0.4127 - val_acc: 0.8933\n",
      "Epoch 861/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0360 - acc: 0.9970 - val_loss: 0.4035 - val_acc: 0.8933\n",
      "Epoch 862/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0429 - acc: 0.9941 - val_loss: 0.3982 - val_acc: 0.8978\n",
      "Epoch 863/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0427 - acc: 0.9941 - val_loss: 0.4247 - val_acc: 0.8800\n",
      "Epoch 864/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0449 - acc: 0.9956 - val_loss: 0.4883 - val_acc: 0.8533\n",
      "Epoch 865/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0503 - acc: 0.9896 - val_loss: 0.4659 - val_acc: 0.8711\n",
      "Epoch 866/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0398 - acc: 0.9970 - val_loss: 0.4320 - val_acc: 0.8800\n",
      "Epoch 867/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0367 - acc: 1.0000 - val_loss: 0.4216 - val_acc: 0.8844\n",
      "Epoch 868/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0421 - acc: 0.9941 - val_loss: 0.4300 - val_acc: 0.8800\n",
      "Epoch 869/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0446 - acc: 0.9956 - val_loss: 0.4461 - val_acc: 0.8756\n",
      "Epoch 870/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0497 - acc: 0.9881 - val_loss: 0.4272 - val_acc: 0.8800\n",
      "Epoch 871/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0396 - acc: 0.9985 - val_loss: 0.4196 - val_acc: 0.8844\n",
      "Epoch 872/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0375 - acc: 0.9970 - val_loss: 0.4191 - val_acc: 0.8844\n",
      "Epoch 873/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0502 - acc: 0.9867 - val_loss: 0.4527 - val_acc: 0.8667\n",
      "Epoch 874/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0486 - acc: 0.9896 - val_loss: 0.4682 - val_acc: 0.8578\n",
      "Epoch 875/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0396 - acc: 0.9941 - val_loss: 0.4124 - val_acc: 0.8844\n",
      "Epoch 876/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0427 - acc: 0.9941 - val_loss: 0.3936 - val_acc: 0.9067\n",
      "Epoch 877/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0410 - acc: 0.9926 - val_loss: 0.3968 - val_acc: 0.8978\n",
      "Epoch 878/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0422 - acc: 1.0000 - val_loss: 0.4303 - val_acc: 0.8711\n",
      "Epoch 879/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0477 - acc: 0.9926 - val_loss: 0.4442 - val_acc: 0.8622\n",
      "Epoch 880/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.0471 - acc: 0.9911 - val_loss: 0.4293 - val_acc: 0.8756\n",
      "Epoch 881/1000\n",
      "675/675 [==============================] - 0s 27us/step - loss: 0.0425 - acc: 0.9956 - val_loss: 0.4139 - val_acc: 0.8800\n",
      "Epoch 882/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0450 - acc: 0.9956 - val_loss: 0.4061 - val_acc: 0.8800\n",
      "Epoch 883/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0405 - acc: 0.9926 - val_loss: 0.4330 - val_acc: 0.8667\n",
      "Epoch 884/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0403 - acc: 0.9896 - val_loss: 0.4623 - val_acc: 0.8533\n",
      "Epoch 885/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0408 - acc: 0.9956 - val_loss: 0.4303 - val_acc: 0.8711\n",
      "Epoch 886/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0429 - acc: 0.9911 - val_loss: 0.4080 - val_acc: 0.8844\n",
      "Epoch 887/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0432 - acc: 0.9926 - val_loss: 0.4189 - val_acc: 0.8844\n",
      "Epoch 888/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0438 - acc: 0.9926 - val_loss: 0.4225 - val_acc: 0.8756\n",
      "Epoch 889/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0427 - acc: 0.9941 - val_loss: 0.4130 - val_acc: 0.8844\n",
      "Epoch 890/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0446 - acc: 0.9956 - val_loss: 0.4175 - val_acc: 0.8889\n",
      "Epoch 891/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0380 - acc: 1.0000 - val_loss: 0.4286 - val_acc: 0.8800\n",
      "Epoch 892/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0426 - acc: 0.9941 - val_loss: 0.4166 - val_acc: 0.8844\n",
      "Epoch 893/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0402 - acc: 0.9956 - val_loss: 0.4228 - val_acc: 0.8800\n",
      "Epoch 894/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0425 - acc: 0.9941 - val_loss: 0.4315 - val_acc: 0.8622\n",
      "Epoch 895/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0374 - acc: 0.9956 - val_loss: 0.4261 - val_acc: 0.8711\n",
      "Epoch 896/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0412 - acc: 0.9941 - val_loss: 0.4221 - val_acc: 0.8800\n",
      "Epoch 897/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0409 - acc: 0.9926 - val_loss: 0.4192 - val_acc: 0.8844\n",
      "Epoch 898/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0427 - acc: 0.9926 - val_loss: 0.4149 - val_acc: 0.8800\n",
      "Epoch 899/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0450 - acc: 0.9926 - val_loss: 0.4285 - val_acc: 0.8756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 900/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0367 - acc: 0.9970 - val_loss: 0.4361 - val_acc: 0.8800\n",
      "Epoch 901/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0456 - acc: 0.9911 - val_loss: 0.4168 - val_acc: 0.8844\n",
      "Epoch 902/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0379 - acc: 0.9956 - val_loss: 0.4060 - val_acc: 0.8933\n",
      "Epoch 903/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0422 - acc: 0.9970 - val_loss: 0.4064 - val_acc: 0.8889\n",
      "Epoch 904/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0361 - acc: 0.9956 - val_loss: 0.4123 - val_acc: 0.8933\n",
      "Epoch 905/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0473 - acc: 0.9896 - val_loss: 0.4248 - val_acc: 0.8844\n",
      "Epoch 906/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0367 - acc: 0.9970 - val_loss: 0.4162 - val_acc: 0.8889\n",
      "Epoch 907/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0419 - acc: 0.9956 - val_loss: 0.4042 - val_acc: 0.8978\n",
      "Epoch 908/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0438 - acc: 0.9926 - val_loss: 0.4096 - val_acc: 0.8933\n",
      "Epoch 909/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0501 - acc: 0.9941 - val_loss: 0.4387 - val_acc: 0.8711\n",
      "Epoch 910/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0407 - acc: 0.9941 - val_loss: 0.4326 - val_acc: 0.8844\n",
      "Epoch 911/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0523 - acc: 0.9896 - val_loss: 0.4003 - val_acc: 0.8933\n",
      "Epoch 912/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0335 - acc: 0.9970 - val_loss: 0.3996 - val_acc: 0.9022\n",
      "Epoch 913/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0411 - acc: 0.9941 - val_loss: 0.4258 - val_acc: 0.8844\n",
      "Epoch 914/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0458 - acc: 0.9926 - val_loss: 0.4554 - val_acc: 0.8711\n",
      "Epoch 915/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0453 - acc: 0.9911 - val_loss: 0.4364 - val_acc: 0.8800\n",
      "Epoch 916/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0381 - acc: 0.9956 - val_loss: 0.4205 - val_acc: 0.8844\n",
      "Epoch 917/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0460 - acc: 0.9881 - val_loss: 0.4219 - val_acc: 0.8933\n",
      "Epoch 918/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0406 - acc: 0.9956 - val_loss: 0.4255 - val_acc: 0.8889\n",
      "Epoch 919/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0386 - acc: 0.9941 - val_loss: 0.4220 - val_acc: 0.8889\n",
      "Epoch 920/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0434 - acc: 0.9926 - val_loss: 0.4030 - val_acc: 0.8933\n",
      "Epoch 921/1000\n",
      "675/675 [==============================] - 0s 21us/step - loss: 0.0381 - acc: 1.0000 - val_loss: 0.3901 - val_acc: 0.8978\n",
      "Epoch 922/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0399 - acc: 0.9926 - val_loss: 0.4055 - val_acc: 0.8889\n",
      "Epoch 923/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0407 - acc: 0.9970 - val_loss: 0.4199 - val_acc: 0.8844\n",
      "Epoch 924/1000\n",
      "675/675 [==============================] - 0s 26us/step - loss: 0.0364 - acc: 0.9956 - val_loss: 0.4437 - val_acc: 0.8667\n",
      "Epoch 925/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0402 - acc: 0.9926 - val_loss: 0.4440 - val_acc: 0.8667\n",
      "Epoch 926/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0390 - acc: 0.9970 - val_loss: 0.4367 - val_acc: 0.8667\n",
      "Epoch 927/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0317 - acc: 0.9970 - val_loss: 0.4116 - val_acc: 0.8844\n",
      "Epoch 928/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0446 - acc: 0.9881 - val_loss: 0.4353 - val_acc: 0.8667\n",
      "Epoch 929/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0362 - acc: 0.9956 - val_loss: 0.4486 - val_acc: 0.8667\n",
      "Epoch 930/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0463 - acc: 0.9881 - val_loss: 0.4197 - val_acc: 0.8933\n",
      "Epoch 931/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0425 - acc: 0.9926 - val_loss: 0.4129 - val_acc: 0.8933\n",
      "Epoch 932/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0468 - acc: 0.9867 - val_loss: 0.4264 - val_acc: 0.8933\n",
      "Epoch 933/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0406 - acc: 0.9926 - val_loss: 0.4239 - val_acc: 0.8933\n",
      "Epoch 934/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0339 - acc: 1.0000 - val_loss: 0.4169 - val_acc: 0.8933\n",
      "Epoch 935/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0362 - acc: 0.9985 - val_loss: 0.4171 - val_acc: 0.8933\n",
      "Epoch 936/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0533 - acc: 0.9881 - val_loss: 0.4301 - val_acc: 0.8800\n",
      "Epoch 937/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0489 - acc: 0.9881 - val_loss: 0.4253 - val_acc: 0.8844\n",
      "Epoch 938/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0425 - acc: 0.9911 - val_loss: 0.4223 - val_acc: 0.8889\n",
      "Epoch 939/1000\n",
      "675/675 [==============================] - 0s 25us/step - loss: 0.0353 - acc: 0.9941 - val_loss: 0.4333 - val_acc: 0.8711\n",
      "Epoch 940/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0364 - acc: 0.9956 - val_loss: 0.4413 - val_acc: 0.8711\n",
      "Epoch 941/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0400 - acc: 0.9941 - val_loss: 0.4439 - val_acc: 0.8711\n",
      "Epoch 942/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0321 - acc: 0.9985 - val_loss: 0.4503 - val_acc: 0.8667\n",
      "Epoch 943/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0329 - acc: 0.9985 - val_loss: 0.4359 - val_acc: 0.8756\n",
      "Epoch 944/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0430 - acc: 0.9881 - val_loss: 0.4391 - val_acc: 0.8667\n",
      "Epoch 945/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0394 - acc: 0.9911 - val_loss: 0.4317 - val_acc: 0.8711\n",
      "Epoch 946/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0427 - acc: 0.9896 - val_loss: 0.4245 - val_acc: 0.8711\n",
      "Epoch 947/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0454 - acc: 0.9911 - val_loss: 0.4383 - val_acc: 0.8667\n",
      "Epoch 948/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0381 - acc: 0.9926 - val_loss: 0.4504 - val_acc: 0.8667\n",
      "Epoch 949/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0433 - acc: 0.9896 - val_loss: 0.4367 - val_acc: 0.8756\n",
      "Epoch 950/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0391 - acc: 0.9926 - val_loss: 0.4167 - val_acc: 0.8933\n",
      "Epoch 951/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0309 - acc: 0.9985 - val_loss: 0.4015 - val_acc: 0.8978\n",
      "Epoch 952/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0449 - acc: 0.9896 - val_loss: 0.4059 - val_acc: 0.8933\n",
      "Epoch 953/1000\n",
      "675/675 [==============================] - 0s 24us/step - loss: 0.0452 - acc: 0.9926 - val_loss: 0.4203 - val_acc: 0.8844\n",
      "Epoch 954/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0335 - acc: 0.9970 - val_loss: 0.4191 - val_acc: 0.8844\n",
      "Epoch 955/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0319 - acc: 0.9956 - val_loss: 0.4184 - val_acc: 0.8844\n",
      "Epoch 956/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0421 - acc: 0.9911 - val_loss: 0.4041 - val_acc: 0.8889\n",
      "Epoch 957/1000\n",
      "675/675 [==============================] - 0s 22us/step - loss: 0.0326 - acc: 0.9985 - val_loss: 0.4065 - val_acc: 0.8933\n",
      "Epoch 958/1000\n",
      "675/675 [==============================] - 0s 23us/step - loss: 0.0396 - acc: 0.9896 - val_loss: 0.4213 - val_acc: 0.8889\n",
      "Epoch 00958: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "'''\n",
    "The validation data split specifies the split of training data between the training set and the cross validation set. \n",
    "For example if the training data size is 1000 then the model uses 750 inputs to train and 250 to validate.\n",
    "'''\n",
    "validation_data_split = 0.25\n",
    "'''\n",
    "The number of epochs determine how many times the data is passed on forward and backward. For each epoch the data is\n",
    "passed forward and backward once.\n",
    "\n",
    "Batch size determines the size of the batch to be processed. The entire data cannot be passed onto the neural network.\n",
    "Decreasing the batch size will increase the time taken to learn.\n",
    "'''\n",
    "num_epochs = 1000\n",
    "model_batch_size = 256\n",
    "tb_batch_size = 6\n",
    "'''\n",
    " It is the number of epochs with no improvement after which training will be stopped.\n",
    "'''\n",
    "early_patience = 100\n",
    "\n",
    "\n",
    "'''\n",
    "TensorBoard is a visualization tool provided with TensorFlow.\n",
    "This callback writes a log for TensorBoard, which allows you to visualize graphs.\n",
    "\n",
    "\n",
    "The Early stopping callback is kind of a monitoring tool which keeps on monitoring a value and stops if there is no \n",
    "improvement. Here we monitor the val_loss.\n",
    "'''\n",
    "tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True)\n",
    "earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min')\n",
    "\n",
    "# Read Dataset\n",
    "dataset = pd.read_csv('training.csv')\n",
    "\n",
    "# Process Dataset\n",
    "processedData, processedLabel = processData(dataset)\n",
    "'''\n",
    "We fit the model with the parameters we specified above.\n",
    "'''\n",
    "history = model.fit(processedData\n",
    "                    , processedLabel\n",
    "                    , validation_split=validation_data_split\n",
    "                    , epochs=num_epochs\n",
    "                    , batch_size=model_batch_size\n",
    "                    , callbacks = [tensorboard_cb,earlystopping_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue>Training and Validation Graphs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x1c3c759748>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1c3c8500b8>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1c3cc58278>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1c3ebcf438>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df = pd.DataFrame(history.history)\n",
    "df.plot(subplots=True, grid=True, figsize=(10,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue>Testing Accuracy [Software 2.0]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeLabel(encodedLabel):\n",
    "    '''\n",
    "    This method does the decoding part (ie) the exact opposite of what encode method did.\n",
    "    '''\n",
    "    if encodedLabel == 0:\n",
    "        return \"Other\"\n",
    "    elif encodedLabel == 1:\n",
    "        return \"Fizz\"\n",
    "    elif encodedLabel == 2:\n",
    "        return \"Buzz\"\n",
    "    elif encodedLabel == 3:\n",
    "        return \"FizzBuzz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 2  Correct :98\n",
      "Testing Accuracy: 98.0\n"
     ]
    }
   ],
   "source": [
    "wrong   = 0\n",
    "right   = 0\n",
    "\n",
    "'''\n",
    "We read the testing data for the model to predict.\n",
    "'''\n",
    "testData = pd.read_csv('testing.csv')\n",
    "\n",
    "'''\n",
    "Following the same procedure we did as with training the data.(Pre-Processing)\n",
    "'''\n",
    "processedTestData  = encodeData(testData['input'].values)\n",
    "processedTestLabel = encodeLabel(testData['label'].values)\n",
    "predictedTestLabel = []\n",
    "\n",
    "'''\n",
    "For all test data points we predict the output using the model we just built and learnt.\n",
    "'''\n",
    "for i,j in zip(processedTestData,processedTestLabel):\n",
    "    y = model.predict(np.array(i).reshape(-1,10))\n",
    "    predictedTestLabel.append(decodeLabel(y.argmax()))\n",
    "    '''\n",
    "    We find out which class has the maximum value\n",
    "    '''\n",
    "    if j.argmax() == y.argmax():\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "\n",
    "print(\"Testing Accuracy: \" + str(right/(right+wrong)*100))\n",
    "\n",
    "# Please input your UBID and personNumber \n",
    "testDataInput = testData['input'].tolist()\n",
    "testDataLabel = testData['label'].tolist()\n",
    "\n",
    "testDataInput.insert(0, \"UBID\")\n",
    "testDataLabel.insert(0, \"vharisha\")\n",
    "\n",
    "testDataInput.insert(1, \"personNumber\")\n",
    "testDataLabel.insert(1, \"50291399\")\n",
    "\n",
    "predictedTestLabel.insert(0, \"\")\n",
    "predictedTestLabel.insert(1, \"\")\n",
    "'''\n",
    "We concatenate all the data we just obtained and we put it in a csv file using the pandas library.\n",
    "'''\n",
    "output = {}\n",
    "output[\"input\"] = testDataInput\n",
    "output[\"label\"] = testDataLabel\n",
    "\n",
    "output[\"predicted_label\"] = predictedTestLabel\n",
    "\n",
    "opdf = pd.DataFrame(output)\n",
    "opdf.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
